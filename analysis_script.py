# -*- coding: utf-8 -*-
"""Copy of Copy of Copy of uncertanty_inhibition_with_15_shape_3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13uZGzaEPHXNGBuwLA3Rl1NQ8VLj3vbcW
"""



"""## install libraries"""

!pip install pingouin
!pip install regressors
!pip install bioinfokit==2.0.8

from google.colab.data_table import DataTable
import sys
import pandas as pd
import os
import glob
import numpy as np
from scipy.stats import ttest_rel
from scipy.stats import ttest_1samp
from scipy.stats import pearsonr
import seaborn as sn
import matplotlib.pyplot as plt
from bioinfokit.analys import stat
import pingouin as pt
from statsmodels.stats.anova import AnovaRM

"""# create data

##load data from chrome
"""

from google.colab import data_table
from google.colab import drive
DataTable.max_columns = 80
drive.mount('/content/drive')
path='/content/drive/MyDrive/data-research'

"""## const veriable"""

file_part_1 = "file_part_1"
true = 1.0
false = 0.0
experiment = "experiment"
stroop_corr = "stroop_anser.corr"
common = "common"
true_shape = "true_shaps"
file_part_2 = "expriment_file"
congruent = "congruent"
not_image_file_part_2 = "not_image"
cerainty_file_part_2 = "cerainty"
uncertainty_file_part_2 = "uncertainty"
part_1 = "part_1"
answer_test_currect = 'test_uncertainty_anser_currect'
click_name = "test_mouse_anser_rual.clicked_name"
infrequnce_error = 'infrequnce_error'
stress = "stress"
anxiety = "anxiety"
general = "general"
participant = "participant"
click_place_name = "test_certainty_mouse.clicked_name"

"""## load data from file - csv (part 1)"""

path_file = path + "/" + file_part_1
csv_files = glob.glob(os.path.join(path_file, "*.csv"))
dfs1 = pd.concat([pd.read_csv(f,na_values="").fillna(value = 0) for f in  csv_files])
dfs1 = dfs1[(dfs1['experiment'] == "certainty")]
# drop pilot participant
dfs1 = dfs1[(dfs1['participant'] !=12) & (dfs1['participant'] !=13)]
dfs1.tail(4)

"""## load data from file - xslx (part 2)"""

path_all = path + "/" + file_part_2
csv_files = glob.glob(os.path.join(path_all, "*.xlsx"))
dfs2 = pd.concat([pd.read_excel(f, sheet_name=f.split("/")[-1].split(".")[0],na_values="").fillna(value = 0) for f in csv_files])
# drop pilot participant
dfs2 = dfs2[(dfs2['participant'] != 12) & (dfs2['participant'] !=13)]
dfs2_1 = dfs2[dfs2[experiment] == "uncertainty"]
dfs2_2 = dfs2[dfs2[experiment] == "certainty"]
dfs2_1.tail(4)

dfs2_2.tail(4)

"""#graph

## kdeplot
"""

def kdeplot(name1, name2, a, b, title):
  sn.kdeplot(data=a, shade=True, color="red", label=name1)
  sn.kdeplot(data=b, shade=True, color="blue", label=name2)
  plt.title(title)
  plt.legend()
  plt.show()

"""##barplot"""

def barplot(c1, c2, bar1, bar2, title):
    SMALL_SIZE = 15
    MEDIUM_SIZE = 15
    BIGGER_SIZE = 15
    plt.rcParams['figure.facecolor'] = "dimgrey"
    plt.rc('font', size=SMALL_SIZE)
    plt.rc('axes', titlesize=SMALL_SIZE)
    plt.rc('axes', titlesize=SMALL_SIZE)  # fontsize of the axes title
    plt.rc('axes', labelsize=MEDIUM_SIZE)  # fontsize of the x and y labels
    plt.rc('xtick', labelsize=SMALL_SIZE)  # fontsize of the tick labels
    plt.rc('ytick', labelsize=SMALL_SIZE)  # fontsize of the tick labels
    plt.rc('legend', fontsize=SMALL_SIZE)  # legend fontsize
    plt.rc('figure', titlesize=BIGGER_SIZE)  # fontsize of the figure title
    plt.tick_params(labelsize=15)
    c = ['steelblue', 'lightpink','limegreen',  'blue']
    plt.bar([c1,c2],[bar1.mean(),bar2.mean()],width=0.4, color=c, alpha=0.7)
    # plt.rcParams['figure.set_alpha'] = 0.1
    plt.title(title)
    
    plt.box(on=None)
    plt.ylabel("Error Rate")
    n = len(bar1.axes[0])
    std = ((np.std(bar1 - bar2)/np.sqrt(n))) * (n/(n-1))
    std1, std2 = bar1.std(), bar2.std()
    n1, n2 = len(bar1), len(bar2)
    se1, se2 = std1/np.sqrt(n1), std2/np.sqrt(n2)
    sed = np.sqrt(se1**2.0 + se2**2.0)
    # print(std1, std2, n1, n2, se1, se2, sed)
    # print(std)

    plt.errorbar([c1,c2], [bar1.mean(),bar2.mean()], yerr=[std, std], fmt="o", color="black")
    plt.show()
    plt.rcParams['figure.facecolor'] = "white"

"""##graphs for anova test after melt"""

def anovagraphs(dfs_anova, col_v='common', x_val="congruent", value='value'):
    sn.catplot(kind='bar',
           data=dfs_anova,
           x=x_val, hue=col_v, y=value)
    sn.catplot(kind='box',
           data=dfs_anova,
           x=x_val, hue=col_v, y=value)
    sn.catplot(kind='point',
           data=dfs_anova,
            x=x_val, hue=col_v, y=value)
    sn.catplot(kind='strip',
           data=dfs_anova,
            x=x_val, col=col_v, y=value, hue='participant')
    sn.catplot(kind='point',
           data=dfs_anova,
            x=x_val, col=col_v, y=value, hue='participant')

"""# Stroop

## accuracy stroop
"""

def accuracyStroop(dfs, block):
  accuracy_stroop = dfs[(dfs['stroop_anser.corr'] == true) & (dfs['experiment'] != 0)].pivot_table( 
    index=["participant"], columns=[congruent], values=[], aggfunc=[len])
  accuracy_stroop.columns = accuracy_stroop.columns.droplevel(0)
  a = accuracy_stroop[0.0]
  b = accuracy_stroop[1.0]
  print('0: ', a.mean(), a.std())
  print('1: ', b.mean(), b.std())
  res = stat()
  res.ttest(accuracy_stroop, res=[0, 1], test_type=3)
  print(res.summary)
  kdeplot("incongruent", "congrunet", a, b, "stroop accuracy: block " + str(block))
  barplot("incongruent", "congrunet", a, b, "stroop accuracy: block " + str(block))
  return accuracy_stroop

"""## time stroop"""

def timeStroop(dfs, block):
  time_stroop = dfs[(dfs['stroop_anser.corr'] == true) & (dfs['experiment'] != 0)].pivot_table( 
    index=["participant"], columns=[congruent], values=['stroop_anser.rt'], aggfunc=[np.mean])
  time_stroop.columns = time_stroop.columns.droplevel(0)
  time_stroop.columns = time_stroop.columns.droplevel(0)
  a = time_stroop[0.0]
  b = time_stroop[1.0]
  print('0: ', a.mean(), a.std())
  print('1: ', b.mean(), b.std())
  res = stat()
  res.ttest(time_stroop, res=[0, 1], test_type=3)
  print(res.summary)

  kdeplot("incongruent", "congrunet", a, b, "stroop time: block " + str(block))
  barplot("incongruent", "congrunet", a, b, "stroop time: block " + str(block))
  return time_stroop

dfs1_accuracy_stroop = accuracyStroop(dfs1, 1)
dfs1_time_stroop = timeStroop(dfs1, 1)

dfs2_1_accuracy_stroop = accuracyStroop(dfs2_1, 2.1)
dfs2_1_time_stroop = timeStroop(dfs2_1, 2.1)

dfs2_2_accuracy_stroop = accuracyStroop(dfs2_2, 2.2)
dfs2_2_time_stroop = timeStroop(dfs2_2, 2.2)

dfs_all = dfs1.append(dfs2_1, ignore_index=True)
dfs_all = dfs_all.append(dfs2_2, ignore_index=True)
dfs_all_accuracy_stroop = accuracyStroop(dfs_all, 'all')
dfs_all_time_stroop = timeStroop(dfs_all, 'all')

"""## error per participant"""

num_std_dist = 3
all_test = pd.concat([dfs1_accuracy_stroop, dfs2_1_accuracy_stroop, dfs2_2_accuracy_stroop], axis=1, ignore_index=True)
stroop_result_all = all_test.sum(axis=1) / 3
mean_result, std_result = stroop_result_all.mean(), stroop_result_all.std()
print("mean: " + str(mean_result) + " std: " + str(std_result))
stroop_result_all.plot.bar(x='participant')
plt.hlines(mean_result - num_std_dist*std_result, 0, 100, color='black')
plt.show()

stroop_result_1 = dfs1_accuracy_stroop.sum(axis=1)
mean_result, std_result = stroop_result_1.mean(), stroop_result_1.std()
print("mean: " + str(mean_result) + " std: " + str(std_result))
stroop_result_1.plot.bar(x='participant',)
plt.hlines(mean_result - num_std_dist*std_result, 0, 100, color='black')
plt.show()

stroop_result_2_1 = dfs2_1_accuracy_stroop.sum(axis=1)
mean_result, std_result = stroop_result_2_1.mean(), stroop_result_2_1.std()
print("mean: " + str(mean_result) + " std: " + str(std_result))
stroop_result_2_1.plot.bar(x='participant',)
plt.hlines(mean_result - num_std_dist*std_result, 0, 100, color='black')
plt.show()

stroop_result_2_2 = dfs2_2_accuracy_stroop.sum(axis=1)
mean_result, std_result = stroop_result_2_2.mean(), stroop_result_2_2.std()
print("mean: " + str(mean_result) + " std: " + str(std_result))
stroop_result_2_2.plot.bar(x='participant',)
plt.hlines(mean_result - num_std_dist*std_result, 0, 100, color='black')
plt.show()

"""# cleanup data - without stroop error and without trail"""

dfs1 = dfs1[(dfs1['stroop_anser.corr'] == true) & (dfs1['experiment'] != 0)]
dfs2_1 = dfs2_1[(dfs2_1['stroop_anser.corr'] == true) & (dfs2_1['experiment'] != 0)]              
dfs2_2 = dfs2_2[(dfs2_2['stroop_anser.corr'] == true) & (dfs2_2['experiment'] != 0)]

dfs2_1_accurcy1 = dfs2_1.pivot_table(index=["participant"], 
                                 columns=["test_uncertainty_anser_currect"], values=[],
                                                                 aggfunc=[len])
dfs2_2_accurcy1 = dfs2_2.pivot_table(index=["participant"], 
                                columns=["test_uncertainty_anser_currect"], values=[],
                                aggfunc=[len])
dfs2_accurcy_rate = (dfs2_1_accurcy1 + dfs2_2_accurcy1)
def succsesRare(x):
  number_of_all =  (x[('len', 1)] + x[('len',    0)]);
  prec = (x[('len', 1)] / number_of_all)
  print("mean:")
  display(prec.mean())
  print("std:")
  display(prec.std())

succsesRare(dfs2_accurcy_rate)
succsesRare(dfs2_1_accurcy1)
succsesRare(dfs2_2_accurcy1)

"""# part 1

## accurcy as function of common
"""

dfs1_accurcy = dfs1[dfs1["test_certainty_anser_currect"] == True].pivot_table(index=["participant"], 
                                columns=[congruent, common, "test_certainty_anser_currect"], values=[],
                                aggfunc=[len])
dfs1_accurcy.columns = dfs1_accurcy.columns.droplevel([0, 3])
x = dfs1[dfs1["test_certainty_anser_currect"] == True].pivot_table(index=["participant"], 
                                columns=["test_certainty_anser_currect"], values=[],
                                aggfunc=[len])
dfs1_accurcy_mean = pd.DataFrame({"mean": dfs1_accurcy.mean(), "std": dfs1_accurcy.std()})
sn.catplot(kind='bar', data=dfs1_accurcy.mean().reset_index(),
           col="common", x='congruent', y=0)
dfs1_accurcy_mean

"""## time as function of common """

dfs1_time = dfs1[(dfs1["test_certainty_anser_currect"] == True)].pivot_table(index=["participant"], 
                                columns=[congruent, common], values=["test_certainty_anser_time"],
                                aggfunc=[np.mean])
dfs1_time.columns = dfs1_time.columns.droplevel(level=[0,1])
display(dfs1_time.mean())
display(dfs1_time.std())
dfs1_time_mean = pd.DataFrame({"mean": dfs1_time.mean(), "std": dfs1_time.std()})
sn.catplot(kind='bar', data=dfs1_time.mean().reset_index(),
           col="common", x='congruent', y=0)
plt.show()

time_for_infreq_dfs1 = dfs1_time.xs("infrequent", level=common, axis=1).reset_index(drop=True)
sn.catplot(kind='bar', data=time_for_infreq_dfs1.mean().reset_index(),
            x='congruent', y=0)
plt.title("time_for_infreq_dfs1")
plt.show();
display(ttest_rel(time_for_infreq_dfs1[0.0], time_for_infreq_dfs1[1.0]))

time_for_widespread_dfs1 = dfs1_time.xs("widespread", level=common, axis=1).reset_index(drop=True)
display((time_for_widespread_dfs1[0.0] - time_for_widespread_dfs1[1.0]).mean())
sn.catplot(kind='bar', data=time_for_widespread_dfs1.mean().reset_index(),
            x='congruent', y=0)
plt.title("time_for_widespread_dfs1")
plt.show();
display(ttest_rel(time_for_widespread_dfs1[0.0], time_for_widespread_dfs1[1.0]))

time_for_dfs1 = dfs1[dfs1["test_certainty_anser_currect"] == True].pivot_table(index=["participant"], 
                                columns=[congruent], values=["test_certainty_mouse.time"],
                                aggfunc=[np.mean]).reset_index(drop=True)
time_for_dfs1.columns = time_for_dfs1.columns.droplevel(level=[0,1])
sn.catplot(kind='bar', data=time_for_dfs1.mean().reset_index(),
            x='congruent', y=0)
plt.title('time_for_dfs1')
plt.show();                    
display(ttest_rel(time_for_dfs1[0.0], time_for_dfs1[1.0]))

display(dfs1[(dfs1["test_certainty_anser_currect"] == True) & (dfs1[participant] == 36) & (dfs1["test_certainty_mouse.time"] != 0)])

"""## cerate standart error on ber plot"""

def create_se_on_graph(x, se):
  for j, i in enumerate(x.facet_axis(0,0).patches):
    xpoint = [i.get_x() + (i.get_width()/2), i.get_x() + (i.get_width()/2)]
    ypoint = [i.get_height()-se[j], i.get_height()+se[j]]
    plt.plot(xpoint, ypoint,color="black", linewidth=3.0, alpha=0.6)

"""## anova accurcy as function of common & stroop"""

'''
this is a % of accurcy
'''
x = dfs1.pivot_table(index=["participant"], values=[],
                                columns=[congruent, common],
                                aggfunc=[len])
x.columns = x.columns.droplevel(level=[0])
dfs1_accurcy_div = dfs1_accurcy / x

'''
mean of effect common
'''
main_effect_common = dfs1[dfs1["test_certainty_anser_currect"] == True].pivot_table(index=["participant"], 
                                columns=[common, "test_certainty_anser_currect"], values=[],
                                aggfunc=[len])
main_effect_common.columns = main_effect_common.columns.droplevel([0, 2])
x1 = dfs1.pivot_table(index=["participant"], values=[],
                                columns=[common],
                                aggfunc=[len])
main_effect_common_dic  = main_effect_common / x1
display(main_effect_common_dic.mean())
display(main_effect_common_dic.std())

dfs1_accurcy_reset = dfs1_accurcy_div.reset_index()
dfs1_accurcy_melt = pd.melt(dfs1_accurcy_reset,id_vars=[("participant", '')],
              value_vars=[(0.0, 'infrequent'),(0.0, 'widespread'),(1.0, 'infrequent'), (1.0, 'widespread')])
dfs1_accurcy_melt = dfs1_accurcy_melt.rename(columns={("participant", ''): "participant"})

dfs1_accurcy_melt=dfs1_accurcy_melt.rename(columns = {'value':'Accuracy (%)'})

dfs1_accurcy_melt.loc[dfs1_accurcy_melt["congruent"] == 0.0,"congruent"] = 'Incongruent'
dfs1_accurcy_melt.loc[dfs1_accurcy_melt["congruent"] == 1.0, "congruent"] = 'Congruent'

dfs1_accurcy_melt.loc[dfs1_accurcy_melt["common"] == 'widespread',"common"] = 'Common'
dfs1_accurcy_melt.loc[dfs1_accurcy_melt["common"] == 'infrequent', "common"] = 'Rare'

dfs1_accurcy_melt=dfs1_accurcy_melt.rename(columns = {congruent:'Congruency'})
dfs1_accurcy_melt=dfs1_accurcy_melt.rename(columns = {common:'Frequency'})

display(pt.rm_anova(dv='Accuracy (%)', within=['Frequency', 'Congruency'], data=dfs1_accurcy_melt, subject="participant",
             detailed=True))
x = sn.catplot(kind='bar',
           data=dfs1_accurcy_melt, ci =None,
           x='Frequency', hue="Congruency", y='Accuracy (%)')

## base on file data standart error
se = [0.0141, 0.0135, 0.00844, 0.00758]
create_se_on_graph(x, se)

"""## anova time as function of common & stroop"""

main_effect_common = dfs1[(dfs1["test_certainty_anser_currect"] == True)].pivot_table(index=["participant"], 
                                columns=[common], values=["test_certainty_anser_time"],
                                aggfunc=[np.mean])
main_effect_common.columns = main_effect_common.columns.droplevel(level=[0,1])
display("common main", main_effect_common.mean())
display("common std", main_effect_common.std())

main_effect_common = dfs1[(dfs1["test_certainty_anser_currect"] == True)].pivot_table(index=["participant"], 
                                columns=[congruent], values=["test_certainty_anser_time"],
                                aggfunc=[np.mean])
main_effect_common.columns = main_effect_common.columns.droplevel(level=[0,1])
display("inhibition main", main_effect_common.mean())
display("inhibition std", main_effect_common.std())

dfs1_time_reset = dfs1_time.reset_index()                         
dfs1_time_melt = pd.melt(dfs1_time_reset, id_vars=[("participant", '')],
              value_vars=[(0.0, 'infrequent'),(0.0, 'widespread'),(1.0, 'infrequent'), (1.0, 'widespread')])
dfs1_time_melt = dfs1_time_melt.rename(columns={("participant", ''): "participant"})
display(pt.rm_anova(dv='value', within=['common', 'congruent'], data=dfs1_time_melt, subject="participant",
            detailed=True))
dfs1_time_melt=dfs1_time_melt.rename(columns = {'value':'Reaction Time (RT)'})

dfs1_time_melt.loc[dfs1_time_melt["congruent"] == 0.0,"congruent"] = 'Incongruent'
dfs1_time_melt.loc[dfs1_time_melt["congruent"] == 1.0, "congruent"] = 'Congruent'

dfs1_time_melt=dfs1_time_melt.rename(columns = {congruent:'Congruency'})
dfs1_time_melt.loc[dfs1_time_melt["common"] == 'widespread',"common"] = 'Common'
dfs1_time_melt.loc[dfs1_time_melt["common"] == 'infrequent', "common"] = 'Rare'

dfs1_time_melt=dfs1_time_melt.rename(columns = {congruent:'Congruency'})
dfs1_time_melt=dfs1_time_melt.rename(columns = {common:'Frequency'})

x = sn.catplot(kind='bar',
           data=dfs1_time_melt, ci =None,
           x='Frequency', hue="Congruency", y='Reaction Time (RT)')
se = [0.0879495, 0.088717, 0.06319, 0.0585]
create_se_on_graph(x, se)

"""## coraltion with question"""

dfs1_time1 = dfs1[(dfs1["test_certainty_anser_currect"] == True)].pivot_table(index=["participant"], 
                                columns=[], values=["test_certainty_anser_time"],
                                aggfunc=[np.mean])
dfs1_time1.columns = dfs1_time1.columns.droplevel(level=[0])
def corraltion1(data, name):
  path_f = path + "/inhibitory_control&uncertainty&check_score.xlsx"
  QR_score = pd.concat([pd.read_excel(path_f, sheet_name='Sheet0',na_values="").fillna(value = 0)])
  # pilot and drop out participant
  for i in [52, 20, 41, 12, 13]:
    QR_score = QR_score.drop(QR_score[QR_score['participant'] == i].index)
  QR_score = QR_score.sort_values(by=['participant'], ignore_index=True)
  a = pd.DataFrame(QR_score).merge(data, how='left', on='participant')
  for i in ['depression', 'anxiety', 'stress', 'OCI-R', 'IUS']:
      test_value = i
      #a=a.rename(columns = {'test_certainty_anser_time':'Reaction Time (RT)'})
      #a=a.rename(columns = {'infrequent':'Reaction Time (RT)'})
      #name = 'Reaction Time (RT)'
      # sn.catplot(data=a,
      #          x=name, y=test_value)
      sn.regplot(data=a, ci=None,
                x=name, y=test_value)
      plt.show()
      print(i)
      display(pearsonr(a[test_value], a[name]))
corraltion1(dfs1_time1, 'test_certainty_anser_time')

"""##corraltion RT to true & rare"""

x = dfs1[(dfs1["test_certainty_anser_currect"] == True)].pivot_table(index=["participant"], 
                                columns=[common], values=["test_certainty_anser_time"],
                                aggfunc=[np.mean])
x.columns = x.columns.droplevel(level=[0, 1])
x.drop('widespread', inplace=True, axis=1)
corraltion1(x, 'infrequent')

"""## corraltion for RT to true & common """

x = dfs1[(dfs1["test_certainty_anser_currect"] == True)].pivot_table(index=["participant"], 
                                columns=[common], values=["test_certainty_anser_time"],
                                aggfunc=[np.mean])
x.columns = x.columns.droplevel(level=[0, 1])
x.drop('infrequent', inplace=True, axis=1)
corraltion1(x, 'widespread')

"""# part 2.1

## accurracy as function of common - data
"""

display(dfs2_1[dfs2_1["test_uncertainty_anser_currect"] == True].pivot_table(index=["participant"], 
                                                  columns=[common, congruent],values=[], aggfunc=[len]))

"""## error as function of common"""

dfs2_1_accurcy = dfs2_1[dfs2_1["test_uncertainty_anser_currect"] == False].pivot_table(index=["participant"], 
                                columns=[congruent, common, "test_uncertainty_anser_currect"], values=[],
                                aggfunc=[len])
dfs2_1_accurcy.columns = dfs2_1_accurcy.columns.droplevel([0, 3])

display(pd.DataFrame({"mean":dfs2_1_accurcy.mean(), "std": dfs2_1_accurcy.std()}))
sn.catplot(kind='bar', data=dfs2_1_accurcy.mean().reset_index(),
           col="common", x='congruent', y=0)

"""## time as fonction of common & stroop"""

dfs2_1_time = (dfs2_1[dfs2_1["test_uncertainty_anser_currect"] == True]).pivot_table(index=["participant"], 
                                columns=[congruent, common], values=["test_mouse_anser_rual.time"],
                                aggfunc=[np.mean])
dfs2_1_time.columns = dfs2_1_time.columns.droplevel(level=[0,1])

dfs2_1_time_mean = pd.DataFrame({"mean": dfs2_1_time.mean(), "std": dfs2_1_time.std()})
sn.catplot(kind='bar', data=dfs2_1_time.mean().reset_index(),
           col="common", x='congruent', y=0)
dfs2_1_time_mean

"""##anova kind of error as function of the currect answer"""

dfs2_1[infrequnce_error] = np.select([(dfs2_1[experiment] == "uncertainty") &
                                        (dfs2_1[click_name] == "three_right")], ["infreq_answer"], default="wides_answer")
dfs2_1.loc[dfs2_1["test_uncertainty_anser_currect"] == True, infrequnce_error] = "true_answer"
display(dfs2_1)
dfs2_1_accurcy = dfs2_1.pivot_table(index=["participant"], 
                                columns=[common, infrequnce_error], values=[],
                                aggfunc=[len])
dfs2_1_accurcy.columns = dfs2_1_accurcy.columns.droplevel(level=[0])
dfs2_1_accurcy = dfs2_1_accurcy.reset_index()
dfs2_1_accurcy_melt = pd.melt(dfs2_1_accurcy, id_vars=[dfs2_1_accurcy.columns[0]],
              value_vars=[dfs2_1_accurcy.columns[i] for i in range(1,len(dfs2_1_accurcy.columns))])
dfs2_1_accurcy_melt = dfs2_1_accurcy_melt.rename(columns={("participant", ''): "participant"})
display(pt.rm_anova(dv='value', within=['common', 'infrequnce_error'], data=dfs2_1_accurcy_melt, subject="participant",
             detailed=True))
anovagraphs(dfs2_1_accurcy_melt, x_val=infrequnce_error)

"""## anova 3-way kind of error as function of currect answer, common & stroop """

mean_kind_answer = dfs2_1.pivot_table(index=["participant"],columns=infrequnce_error,aggfunc=[len], values=[])
mean_kind_answer.columns = mean_kind_answer.columns.droplevel(level=[0])
print(mean_kind_answer.mean())
print(mean_kind_answer.std())
res = stat()
res.ttest(df=mean_kind_answer, test_type=1, mu=6.6, res='true_answer')
print(res.summary)

dfs2_1_accurcy_3_way = dfs2_1.pivot_table(
                                index=["participant"], 
                                columns=[congruent, common, infrequnce_error], values=[],
                                aggfunc=[len], fill_value=0).fillna(0)
dfs2_1_accurcy_3_way.columns = dfs2_1_accurcy_3_way.columns.droplevel(level=[0])

infreq_answer_zero_row = pd.DataFrame([{participant:i,congruent:c, common:'infrequent',
                                        infrequnce_error:'infreq_answer', 'value':0} 
                                       for i in dfs2_1_accurcy_3_way.index for c in [0, 1]])

dfs2_1_accurcy_3_way = dfs2_1_accurcy_3_way.reset_index()
len_of_answer_common = dfs2_1.pivot_table(index=["participant"], 
                                columns=[congruent, common], values=[],
                                aggfunc=[len], fill_value=0).fillna(0)
len_of_answer_common.columns = len_of_answer_common.columns.droplevel(level=[0])

x = dfs2_1_accurcy_3_way
dfs2_1_accurcy_3_way_copy_without_precent = dfs2_1_accurcy_3_way.copy()
s1 = pd.DataFrame(x[(0, 'widespread', 'true_answer')])
x[(0, 'widespread', 'true_answer')]  = (s1.values / pd.DataFrame(len_of_answer_common[(0, "widespread")]).values)
s2 = pd.DataFrame(x[(1, 'widespread', 'true_answer')])
x[(1, 'widespread',   'true_answer')] = (s2.values / pd.DataFrame(len_of_answer_common[(1, "widespread")]).values)
s1 = pd.DataFrame(x[(0, 'infrequent', 'true_answer')])
x[(0, 'infrequent', 'true_answer')]  = (s1.values / pd.DataFrame(len_of_answer_common[(0, "infrequent")]).values)
s2 =pd.DataFrame(x[(1, 'infrequent', 'true_answer')])
x[(1, 'infrequent',   'true_answer')] = (s2.values / pd.DataFrame(len_of_answer_common[(1, "infrequent")]).values)

#display((dfs2_1_accurcy_3_way.xs(('true_answer'), level=infrequnce_error, axis=1)))
#display((dfs2_1_accurcy_3_way.xs(('infreq_answer'), level=infrequnce_error, axis=1)))
#display((dfs2_1_accurcy_3_way.xs(('wides_answer'), level=infrequnce_error, axis=1)))

dfs2_1_accurcy_melt_3_way = pd.melt(dfs2_1_accurcy_3_way, id_vars=[dfs2_1_accurcy_3_way.columns[0]],
              value_vars=[dfs2_1_accurcy_3_way.columns[i] for i in range(1, len(dfs2_1_accurcy_3_way.columns))])
dfs2_1_accurcy_melt_3_way = dfs2_1_accurcy_melt_3_way.rename(columns={("participant", '', ''): "participant"})
dfs2_1_accurcy_melt_3_way = pd.concat([dfs2_1_accurcy_melt_3_way , infreq_answer_zero_row], ignore_index=True)

"""## step 1: anova for true answer : common & conguent (%)"""

dfs2_1_accurcy_melt_3_way_copy = dfs2_1_accurcy_melt_3_way.copy()
data_true_answer_dfs_2_1 = dfs2_1_accurcy_melt_3_way_copy[(dfs2_1_accurcy_melt_3_way_copy[infrequnce_error] == 'true_answer')]
aov = AnovaRM(
    data_true_answer_dfs_2_1,
    depvar='value',
    subject="participant",
    within=['congruent', 'common']
    ).fit()
print(aov)
display(pt.rm_anova(dv='value', within=['common', 'congruent'],
  data=data_true_answer_dfs_2_1,
  subject="participant", detailed=True))
x=dfs2_1_accurcy_melt_3_way_copy.rename(columns = {'value':'Accuracy (%)'})
x=x.rename(columns = {infrequnce_error:'answer'})
x.loc[x["congruent"] == 0.0,"congruent"] = 'Incongruent'
x.loc[x["congruent"] == 1.0, "congruent"] = 'Congruent'
x=x.rename(columns = {congruent:'Congruency'})
x.loc[x["answer"] == 'true_answer',"answer"] = 'true answer'

x.loc[x["common"] == 'widespread',"common"] = 'Common'
x.loc[x["common"] == 'infrequent', "common"] = 'Rare'

x=x.rename(columns = {common:'Frequency'})

z = sn.catplot(kind='bar', 
           data=x[x['answer'] == 'true answer'], ci=None,
           x='Frequency', col='answer', hue='Congruency', y='Accuracy (%)')

se = [0.02767, 0.0302818, 0.029396, 0.02266]
create_se_on_graph(z, se)

"""## steps 2: t-test for widsp & true answer : conguent (%)"""

def ttest_for_true_answer(infrequnce):
  dfs_2_1_true_answer_common_a = dfs2_1_accurcy_melt_3_way_copy[(dfs2_1_accurcy_melt_3_way_copy[infrequnce_error] == 'true_answer') &
                              (dfs2_1_accurcy_melt_3_way_copy[common] == infrequnce)]                   
  mean_true = dfs_2_1_true_answer_common_a.pivot_table(
    index=["participant"], columns=[congruent], values=['value'],fill_value=0)
  display('mean:', mean_true.mean())
  display('std:', mean_true.std())

  aov = AnovaRM(
    dfs_2_1_true_answer_common_a,
    depvar='value',
    subject="participant",
    within=['congruent']
    ).fit()
  print(aov)
  display(pt.pairwise_tests(data=dfs_2_1_true_answer_common_a, dv='value', 
                         within=['congruent'], subject="participant", effsize='eta-square'))
  dfs_2_1_true_answer_common = dfs_2_1_true_answer_common_a.pivot_table(index=["participant"], 
                                columns=[congruent],values="value").fillna(0)                             
  res = stat()
  res.ttest(dfs_2_1_true_answer_common, res=[0,1], test_type=3)                
  print(res.summary)
  z = sn.catplot(kind='bar', ci=None,
           data= dfs_2_1_true_answer_common)
  # se by file for widspread. 
  se = [0.007844493, 0.00784]
  create_se_on_graph(z, se)

ttest_for_true_answer('widespread')

ttest_for_true_answer('infrequent')

"""## step 3: anova for error - widsp : kind of error & stroop"""

dfs2_1_accurcy_melt_3_way_copy = dfs2_1_accurcy_melt_3_way.copy()
x_2_1 = dfs2_1_accurcy_melt_3_way_copy[(dfs2_1_accurcy_melt_3_way_copy['common'] == "widespread") &
                              (dfs2_1_accurcy_melt_3_way_copy[infrequnce_error] != "true_answer")]
mean_inhibition_pivot = x_2_1.pivot_table(index=["participant"], 
                                columns=[congruent], values=['value'],aggfunc=[sum],
                                fill_value=0)
display(mean_inhibition_pivot.mean())
display(mean_inhibition_pivot.std())
                            
aov = AnovaRM(
    x_2_1,
    depvar='value',
    subject="participant",
    within=['congruent', infrequnce_error]
    ).fit()
print(aov)
display(pt.rm_anova(dv='value', within=['congruent', infrequnce_error],
  data=x_2_1,
  subject="participant", detailed=True))

x=x_2_1.rename(columns = {'value':'Number of Error'})
x=x.rename(columns = {infrequnce_error:'kind of error'})
x.loc[x['kind of error'] == 'infreq_answer',"kind of error"] = 'Uncertainty'
x.loc[x['kind of error'] == 'wides_answer',"kind of error"] = 'Perceptual'
x.loc[x["congruent"] == 0.0,"congruent"] = 'Incongruent'
x.loc[x["congruent"] == 1.0, "congruent"] = 'Congruent' 
x=x.rename(columns = {congruent:'Congruency'})
x=x.rename(columns = {"kind of error":'Error Type'})

z = sn.catplot(kind='bar', ci=None,
           data=x,
           
           col='common', x='Error Type', hue='Congruency', y='Number of Error')
se = [1.7873,1.86053, 1.514233, 2.11886]
create_se_on_graph(z, se)

"""##step 4: ttest for error - widsp & """

def ttest_for_error(error_type):
  x =x_2_1[(x_2_1[infrequnce_error] == error_type)]
  aov = AnovaRM(
    x,
    depvar='value',
    subject="participant",
    within=['congruent']
    ).fit()
  print(aov)

  display(pt.pairwise_tests(data=x, dv='value', 
                         within=['congruent'], subject="participant", effsize='eta-square'))
  x1 = x.pivot_table(index=["participant"], 
                             columns=[congruent],values="value").fillna(0)                          
  display(x1.mean())
  display(x1.std())                           

  res = stat()
  res.ttest(x1, res=[0,1], test_type=3)
  #display((x1[0]-x1[1]).mean()) 
  # display((x1[0]-x1[1]).std() / np.sqrt(np.size(x1[0])))  
  #display((x1[0]-x1[1]).std())  
  #display(x1)          
  print(res.summary)
  x1 = pd.melt(x1)
  #display(x1)
  z = sn.catplot(kind='bar', 
           data=x1,
           ci=None,
           x='congruent', y='value')
  se = [0.412,0.412]
  create_se_on_graph(z, se)

ttest_for_error('infreq_answer')

ttest_for_error('wides_answer')

"""## % of (error say no image) / (say no image) """

p = dfs2_1_accurcy_3_way_copy_without_precent
true_answer_0 = pd.DataFrame(p[(0, 'infrequent', 'true_answer')])
error_0 = pd.DataFrame(p[(0, 'widespread', 'infreq_answer')])
stroop_0 = (error_0.values / (error_0.values + true_answer_0.values)).reshape(1, -1).flatten()

true_answer_1 = pd.DataFrame(p[(1, 'infrequent', 'true_answer')])
error_1 = pd.DataFrame(p[(1, 'widespread', 'infreq_answer')])
stroop_1 = (error_1.values / (error_1.values + true_answer_1.values)).reshape(1, -1).flatten()

present_of_uncertanty = pd.DataFrame([stroop_0, stroop_1]).T

uncertanty_all = pd.DataFrame(((error_1.values+error_0.values) / (error_1.values + true_answer_1.values +
                                                error_0.values + true_answer_0.values)).reshape(1, -1).flatten())
uncertanty_all=uncertanty_all.rename(columns = {0:'uncertanty'})
uncertanty_all = pd.DataFrame(p[participant]).merge(uncertanty_all, left_index=True, right_index=True)
res = stat()
res.ttest(present_of_uncertanty, res=[0,1], test_type=3)  
display(ttest_1samp(a=stroop_0-stroop_1, popmean=0) )           
print(res.summary)
sn.catplot(kind='bar',
           data=present_of_uncertanty)

"""corraltion

## coraltion with question
"""

def corraltion(data, name):
  path_f = path + "/inhibitory_control&uncertainty&check_score.xlsx"
  QR_score = pd.concat([pd.read_excel(path_f, sheet_name='Sheet0',na_values="").fillna(value = 0)])
  for i in [52, 20, 41, 12, 13]:
    QR_score = QR_score.drop(QR_score[QR_score['participant'] == i].index)
  QR_score = QR_score.sort_values(by=['participant'], ignore_index=True)
  a = pd.DataFrame(QR_score).merge(data, how='left', on='participant')
  for i in ['depression', 'anxiety', 'stress', 'OCI-R', 'IUS']:
      test_value = i
      # a=a.rename(columns = {'depression':'Depression'})
      # a=a.rename(columns = {'uncertanty':'Uncertainty Propagation'})
      # name = 'Uncertainty Propagation'
      # test_value = 'Depression'
      sn.catplot(data=a,
                 x=name, y=test_value)
      sn.lmplot(data=a, ci = None,
                 x=name, y=test_value)
      #sn.regplot(data=a, ci=None,
      #          x='Uncertainty Propagation', y=test_value)
    
      plt.show()
      print(i)
      display(pearsonr(a[test_value], a[name]))
corraltion(uncertanty_all, 'uncertanty' )

"""## corraltion - for time - say no image """

dfs2_1_copy = dfs2_1.copy()
dfs2_1_copy[infrequnce_error] = np.select([(dfs2_1_copy[experiment] == "uncertainty") &
                                        (dfs2_1_copy[click_name] == "three_right")], ["infreq_answer"], default="wides_answer")
########### for test only error uncertanty, take the next line
#dfs2_1_copy.loc[dfs2_1["test_uncertainty_anser_currect"] == True, infrequnce_error] = "true_answer"
dfs2_1_time_uncertanty = (dfs2_1_copy.pivot_table(index=["participant"], 
                                columns=[infrequnce_error], values=["test_mouse_anser_rual.time"],
                                aggfunc=[np.mean]))
dfs2_1_time_uncertanty.columns = dfs2_1_time_uncertanty.columns.droplevel(level=[0, 1])
########### for test only error uncertanty, take the next line
# dfs2_1_time_uncertanty.drop('true_answer', inplace=True, axis=1)
dfs2_1_time_uncertanty.drop('wides_answer', inplace=True, axis=1)
corraltion(dfs2_1_time_uncertanty, 'infreq_answer')

"""## corraltion - for time - all kind error"""

dfs2_1_copy[infrequnce_error] = np.select([(dfs2_1_copy[experiment] == "uncertainty") &
                                        (dfs2_1_copy[click_name] == "three_right")], ["error"], default="error")
dfs2_1_copy.loc[dfs2_1["test_uncertainty_anser_currect"] == True, infrequnce_error] = "true_answer"
dfs2_1_time_uncertanty = (dfs2_1_copy.pivot_table(index=["participant"], 
                                columns=[infrequnce_error], values=["test_mouse_anser_rual.time"],
                                aggfunc=[np.mean]))
dfs2_1_time_uncertanty.columns = dfs2_1_time_uncertanty.columns.droplevel(level=[0, 1])
dfs2_1_time_uncertanty.drop('true_answer', inplace=True, axis=1)
dfs2_1_time_uncertanty
corraltion(dfs2_1_time_uncertanty, 'error')

"""## corraltion - accurcy - after inhibtion"""

diffAccurcy = pd.DataFrame(stroop_0 - stroop_1)
diffAccurcy=diffAccurcy.rename(columns = {0:'uncertanty'})
diffAccurcy['participant'] = uncertanty_all['participant']
display(diffAccurcy)
corraltion(diffAccurcy, 'uncertanty')

"""# part 2.2

## accuracy as function of common - data
"""

dfs2_2[dfs2_2["test_uncertainty_anser_currect"] == True].pivot_table(index=["participant"], 
                                                  columns=[common, congruent],values=[], aggfunc=[len])

"""## error as function of common"""

dfs2_2_accurcy = dfs2_2[dfs2_2["test_uncertainty_anser_currect"] == False].pivot_table(index=["participant"], 
                                columns=[congruent, common, "test_uncertainty_anser_currect"], values=[],
                                aggfunc=[len])
dfs2_2_accurcy.columns = dfs2_2_accurcy.columns.droplevel([0, 3])

display(pd.DataFrame({"mean":dfs2_2_accurcy.mean(), "std": dfs2_2_accurcy.std()}))
sn.catplot(kind='bar', data=dfs2_2_accurcy.mean().reset_index(),
           col="common", x='congruent', y=0)

"""## add infrequnce error to data"""

def add_error_of_infrequnce(file):
    file[infrequnce_error] = np.select([(file[experiment] == "certainty") &
                                        ((file[click_name] == "two_up")
                                         | (file[click_name] == "one_up") | (file[click_name] == "three_up") |
                                         (file[click_name] == "four_up") | (file[click_name] == "one_down")
                                         | (file[click_name] == "two_down") | (file[click_name] == "three_down"))],
                                       ["infreq_answer"], default="wides_answer")
    return file
dfs2_2 = add_error_of_infrequnce(dfs2_2)
display(dfs2_2)

"""##anova kind of choose shpae"""

dfs2_2_accurcy = dfs2_2[dfs2_2["test_uncertainty_anser_currect"] == False].pivot_table(index=["participant"], 
                                columns=[common, infrequnce_error], values=[],
                                aggfunc=[len])
dfs2_2_accurcy.columns = dfs2_2_accurcy.columns.droplevel(level=[0])                          
dfs2_2_accurcy = dfs2_2_accurcy.reset_index()
dfs2_2_accurcy_melt = pd.melt(dfs2_2_accurcy, id_vars=[dfs2_2_accurcy.columns[0]],
              value_vars=[dfs2_2_accurcy.columns[i] for i in range(1,len(dfs2_2_accurcy.columns))])
dfs2_2_accurcy_melt = dfs2_2_accurcy_melt.rename(columns={("participant", ''): "participant"})
display(pt.rm_anova(dv='value', within=['common', 'infrequnce_error'], data=dfs2_2_accurcy_melt, subject="participant",
             detailed=True))

anovagraphs(dfs2_2_accurcy_melt, x_val=infrequnce_error)

"""## anova 3-way"""

dfs2_2.loc[dfs2_2["test_uncertainty_anser_currect"] == True, infrequnce_error] = "true_answer"
dfs2_2_accurcy_3_way = dfs2_2.pivot_table(
                                index=["participant"], 
                                columns=[congruent, common, infrequnce_error], values=[],
                                aggfunc=[len], fill_value=0).fillna(0)
u = dfs2_2.pivot_table(
                                index=["participant"], 
                                columns=[common, infrequnce_error], values=[],
                                aggfunc=[len], fill_value=0).fillna(0)

u.columns = u.columns.droplevel(level=[0])
u = u.reset_index()

dfs2_2_accurcy_3_way.columns = dfs2_2_accurcy_3_way.columns.droplevel(level=[0])
dfs2_2_accurcy_3_way = dfs2_2_accurcy_3_way.reset_index()

len_of_answer_common = dfs2_2.pivot_table(index=["participant"], 
                                columns=[congruent, common], values=[],
                                aggfunc=[len], fill_value=0).fillna(0)
len_of_answer_common.columns = len_of_answer_common.columns.droplevel(level=[0])


s1 = pd.DataFrame(u[('widespread', 'true_answer')])
u[('widespread', 'true_answer')]  = (s1.values / (pd.DataFrame(len_of_answer_common[(0, "widespread")]).values + pd.DataFrame(len_of_answer_common[(1, "widespread")]).values))
s1 = pd.DataFrame(u[('infrequent', 'true_answer')])
u[('infrequent', 'true_answer')]  = (s1.values / (pd.DataFrame(len_of_answer_common[(0, "infrequent")]).values + pd.DataFrame(len_of_answer_common[(1, "infrequent")]).values))
print("mean:", u.xs(('true_answer'), level=infrequnce_error, axis=1).mean(axis=0))
print("std:", u.xs(('true_answer'), level=infrequnce_error, axis=1).std(axis=0))
#display(u.xs(('true_answer'), level=infrequnce_error, axis=1))


dfs2_2_accurcy_3_way_copy_without_precent = dfs2_2_accurcy_3_way.copy()
x = dfs2_2_accurcy_3_way
s1 = pd.DataFrame(x[(0, 'widespread', 'true_answer')])
x[(0, 'widespread', 'true_answer')]  = (s1.values / pd.DataFrame(len_of_answer_common[(0, "widespread")]).values)
s2 = pd.DataFrame(x[(1, 'widespread', 'true_answer')])
x[(1, 'widespread',   'true_answer')] = (s2.values / pd.DataFrame(len_of_answer_common[(1, "widespread")]).values)
s1 = pd.DataFrame(x[(0, 'infrequent', 'true_answer')])
x[(0, 'infrequent', 'true_answer')]  = (s1.values / pd.DataFrame(len_of_answer_common[(0, "infrequent")]).values)
s2 =pd.DataFrame(x[(1, 'infrequent', 'true_answer')])
x[(1, 'infrequent',   'true_answer')] = (s2.values / pd.DataFrame(len_of_answer_common[(1, "infrequent")]).values)

#display((dfs2_2_accurcy_3_way.xs(('true_answer'), level=infrequnce_error, axis=1)))

#display((dfs2_2_accurcy_3_way.xs(('infreq_answer'), level=infrequnce_error, axis=1)))
#display((dfs2_2_accurcy_3_way.xs(('wides_answer'), level=infrequnce_error, axis=1)))
#display(dfs2_2_accurcy_3_way)

# find mean
# u = dfs2_2_accurcy_3_way.xs(('true_answer'), level=infrequnce_error, axis=1)
# display(u)

# display(u[(1, "infrequent")] + u[(0, "infrequent")])
# display(u[(1, "widespread")] + u[(0, "widespread")])


dfs2_2_accurcy_melt_3_way = pd.melt(dfs2_2_accurcy_3_way, id_vars=[dfs2_2_accurcy_3_way.columns[0]],
              value_vars=[dfs2_2_accurcy_3_way.columns[i] for i in range(1, len(dfs2_2_accurcy_3_way.columns))])

dfs2_2_accurcy_melt_3_way = dfs2_2_accurcy_melt_3_way.rename(columns={("participant", '', ''): "participant"})
dfs2_2_accurcy_melt_3_way = pd.concat([dfs2_2_accurcy_melt_3_way], ignore_index=True)

aov = AnovaRM(
    dfs2_2_accurcy_melt_3_way[(dfs2_2_accurcy_melt_3_way[infrequnce_error] == 'true_answer')], 
    depvar='value',
    subject="participant",
    within=['congruent', 'common']
    ).fit()
print(aov)

mean = dfs2_2.pivot_table(index=["participant"],columns=infrequnce_error,aggfunc=[len], values=[])
mean.columns = mean.columns.droplevel(level=[0])
print(mean.mean())
print(mean.std())
display(ttest_1samp(mean, 6.6))
res = stat()
res.ttest(df=mean, test_type=1, mu=7.1, res='true_answer')
print(res.summary)

"""## step 1: anova for true answer : common & conguent (%)"""

dfs2_2_accurcy_melt_3_way_copy = dfs2_2_accurcy_melt_3_way.copy()
aov = AnovaRM(
    dfs2_2_accurcy_melt_3_way_copy[(dfs2_2_accurcy_melt_3_way_copy[infrequnce_error] == 'true_answer')],
    depvar='value',
    subject="participant",
    within=['congruent', 'common']
    ).fit()
print(aov)
display(pt.rm_anova(dv='value', within=['congruent', 'common'],
  data=dfs2_2_accurcy_melt_3_way_copy[(dfs2_2_accurcy_melt_3_way_copy[infrequnce_error] == 'true_answer')],
  subject="participant", detailed=True))

x=dfs2_2_accurcy_melt_3_way_copy.rename(columns = {'value':'Accuracy (%)'})
x=x.rename(columns = {infrequnce_error:'answer'})
x.loc[x["answer"] == 'true_answer',"answer"] = 'true answer'
x.loc[x["congruent"] == 0.0,"congruent"] = 'Incongruent'
x.loc[x["congruent"] == 1.0, "congruent"] = 'Congruent'
x=x.rename(columns = {congruent:'Congruency'})

x.loc[x["common"] == 'widespread',"common"] = 'Common'
x.loc[x["common"] == 'infrequent', "common"] = 'Rare'

x=x.rename(columns = {common:'Frequency'})


z = sn.catplot(kind='bar',
           data= x[x['answer'] == 'true answer'], ci=None,
           x='Frequency', col='answer', hue='Congruency', y='Accuracy (%)')

se = [0.02091, 0.01776,0.01422, 0.01193]
create_se_on_graph(z, se)

"""## steps 2: t-test for widsp & true answer : conguent (%)"""

def ttest_true_answer_part_2(infrequent):
  dfs_2_2_true_answer_common_a = dfs2_2_accurcy_melt_3_way_copy[(dfs2_2_accurcy_melt_3_way_copy[infrequnce_error] == 'true_answer') &
                           (dfs2_2_accurcy_melt_3_way_copy[common] == infrequent)]
  aov = AnovaRM(
    dfs_2_2_true_answer_common_a,
    depvar='value',
    subject="participant",
    within=['congruent']
    ).fit()
  print(aov)
  dfs_2_2_true_answer_common = dfs_2_2_true_answer_common_a.pivot_table(index=["participant"], 
                                columns=[congruent],values="value").fillna(0)                               
  res = stat()
  res.ttest(dfs_2_2_true_answer_common, res=[0,1], test_type=3)                
  print(res.summary)
  z = sn.catplot(kind='bar', ci=None,
           data= dfs_2_2_true_answer_common)
  # standart error from file for widespread
  se = [0.00784, 0.00784]
  create_se_on_graph(z, se)

ttest_true_answer_part_2('widespread')

ttest_true_answer_part_2('infrequent')

"""## step 3: anova for error - widsp : kind of error & stroop"""

x_2_2 = dfs2_2_accurcy_melt_3_way[(dfs2_2_accurcy_melt_3_way['common'] == "widespread") &
                              (dfs2_2_accurcy_melt_3_way[infrequnce_error] != "true_answer")]

mean_effect_ifreq_error = x_2_2.pivot_table(index=["participant"], 
                                columns=[infrequnce_error], values=['value'],aggfunc=[sum],
                                fill_value=0)
display(mean_effect_ifreq_error.mean())
display(mean_effect_ifreq_error.std())

aov = AnovaRM(
    x_2_2,
    depvar='value',
    subject="participant",
    within=['congruent', infrequnce_error]
    ).fit()
print(aov)

display(pt.rm_anova(dv='value', within=['congruent', infrequnce_error],
  data=x_2_2,
  subject="participant", detailed=True))

x=x_2_2.rename(columns = {'value':'Number of Error'})
x=x.rename(columns = {infrequnce_error:'kind of error'})
x.loc[x['kind of error'] == 'infreq_answer',"kind of error"] = 'Rare error'
x.loc[x['kind of error'] == 'wides_answer',"kind of error"] = 'Common error'
x.loc[x["congruent"] == 0.0,"congruent"] = 'incongruent'
x.loc[x["congruent"] == 1.0, "congruent"] = 'congruent'

x.loc[x['kind of error'] == 'infreq_answer',"kind of error"] = 'Uncertainty'
x.loc[x['kind of error'] == 'wides_answer',"kind of error"] = 'Perceptual'
x.loc[x["congruent"] == 0.0,"congruent"] = 'Incongruent'
x.loc[x["congruent"] == 1.0, "congruent"] = 'Congruent' 
x=x.rename(columns = {congruent:'Congruency'})
x=x.rename(columns = {"kind of error":'Error Type'})

z = sn.catplot(kind='bar',
           data=x,
           units='participant', ci=None,
           col='common', x='Error Type', hue='Congruency', y='Number of Error')
se = [0.7487, 0.9064, 1.0042, 0.88523]
create_se_on_graph(z, se)

"""##step 4: ttest for error - widsp & """

def ttest_error_part_2(infrequent):
  x2 = x_2_2[(x_2_2[infrequnce_error] == infrequent)]

  aov = AnovaRM(
    x2, 
    depvar='value',
    subject="participant",
    within=['congruent']
    ).fit()
  print(aov)
  x1 = x2.pivot_table(index=["participant"], 
                                columns=[congruent],values="value").fillna(0) 

  res = stat()
  res.ttest(x1, res=[0,1], test_type=3)                
  print(res.summary)

  z = sn.catplot(kind='bar', ci=None,
           data=x2,
           x='congruent', y='value')
  # standart error from file for widspread 
  se = [0.4126, 0.4126]
  create_se_on_graph(z, se)
ttest_error_part_2('wides_answer')

ttest_error_part_2('infreq_answer')

"""# anova part 2.1 & 2.2"""

x_2_1['exp'] = [2.1]*len(x_2_1)
x_2_2['exp'] = [2.2]*len(x_2_2)
connect_data = pd.concat([x_2_1, x_2_2])
aov = AnovaRM(
    connect_data,
    depvar='value',
    subject="participant",
    within=[congruent, infrequnce_error, 'exp']
    ).fit()
print(aov)

sn.catplot(kind='box',
           data=connect_data,
           x='exp', col=infrequnce_error, hue='congruent', y='value')
hue_order = ['true_answer', 'infreq_answer', 'wides_answer']
sn.catplot(kind='box',
           data=dfs2_1_accurcy_melt_3_way,
           x='common', col=infrequnce_error, hue='congruent', col_order=hue_order, y='value')
sn.catplot(kind='box',
           data=dfs2_2_accurcy_melt_3_way,
          x='common', col=infrequnce_error,hue='congruent',col_order=hue_order, y='value')