# -*- coding: utf-8 -*-
"""Copy of Copy of uncertanty_inhibition_with_15_shape_3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1IpgaySac3mLbkjGN2CiDP4J_D4HUTrj8
"""



"""## install libraries"""

!pip install pingouin
!pip install bioinfokit

from google.colab.data_table import DataTable
import sys
import pandas as pd
import os
import glob
import numpy as np
from scipy.stats import ttest_rel
from scipy.stats import ttest_1samp
from scipy.stats import pearsonr
import seaborn as sn
import matplotlib.pyplot as plt
from bioinfokit.analys import stat
import pingouin as pt

"""# create data

##load data from chrome
"""

from google.colab import data_table
from google.colab import drive
DataTable.max_columns = 80
drive.mount('/content/drive')
path='/content/drive/MyDrive/data-research'

"""## const veriable"""

file_part_1 = "file_part_1"
true = 1.0
false = 0.0

experiment = "experiment"
stroop_corr = "stroop_anser.corr"
common = "common"
true_shape = "true_shaps"
file_part_2 = "expriment_file"
congruent = "congruent"
not_image_file_part_2 = "not_image"
cerainty_file_part_2 = "cerainty"
uncertainty_file_part_2 = "uncertainty"
part_1 = "part_1"
answer_test_currect = 'test_uncertainty_anser_currect'
click_name = "test_mouse_anser_rual.clicked_name"
infrequnce_error = 'infrequnce_error'
stress = "stress"
anxiety = "anxiety"
general = "general"
participant = "participant"
click_place_name = "test_certainty_mouse.clicked_name"

"""## load data from file - csv (part 1)"""

path_file = path + "/" + file_part_1
csv_files = glob.glob(os.path.join(path_file, "*.csv"))
dfs1 = pd.concat([pd.read_csv(f,na_values="").fillna(value = 0) for f in  csv_files])
dfs1 = dfs1[(dfs1['experiment'] == "certainty")]
dfs1.tail(4)

"""## load data from file - xslx (part 2)"""

path_all = path + "/" + file_part_2
csv_files = glob.glob(os.path.join(path_all, "*.xlsx"))
dfs2 = pd.concat([pd.read_excel(f, sheet_name=f.split("/")[-1].split(".")[0],na_values="").fillna(value = 0) for f in csv_files])
dfs2_1 = dfs2[dfs2[experiment] == "uncertainty"]
dfs2_2 = dfs2[dfs2[experiment] == "certainty"]
dfs2_1.tail(4)

dfs2_2.tail(4)

"""#graph

## kdeplot
"""

def kdeplot(name1, name2, a, b, title):
  sn.kdeplot(data=a, shade=True, color="red", label=name1)
  sn.kdeplot(data=b, shade=True, color="blue", label=name2)
  plt.title(title)
  plt.legend()
  plt.show()

"""##barplot"""

def barplot(c1, c2, bar1, bar2, title):
    SMALL_SIZE = 15
    MEDIUM_SIZE = 15
    BIGGER_SIZE = 15
    plt.rcParams['figure.facecolor'] = "dimgrey"
    plt.rc('font', size=SMALL_SIZE)
    plt.rc('axes', titlesize=SMALL_SIZE)
    plt.rc('axes', titlesize=SMALL_SIZE)  # fontsize of the axes title
    plt.rc('axes', labelsize=MEDIUM_SIZE)  # fontsize of the x and y labels
    plt.rc('xtick', labelsize=SMALL_SIZE)  # fontsize of the tick labels
    plt.rc('ytick', labelsize=SMALL_SIZE)  # fontsize of the tick labels
    plt.rc('legend', fontsize=SMALL_SIZE)  # legend fontsize
    plt.rc('figure', titlesize=BIGGER_SIZE)  # fontsize of the figure title
    plt.tick_params(labelsize=15)
    c = ['steelblue', 'lightpink','limegreen',  'blue']
    plt.bar([c1,c2],[bar1.mean(),bar2.mean()],width=0.4, color=c, alpha=0.7)
    # plt.rcParams['figure.set_alpha'] = 0.1
    plt.title(title)
    
    plt.box(on=None)
    plt.ylabel("Error Rate")
    n = len(bar1.axes[0])
    std = ((np.std(bar1 - bar2)/np.sqrt(n))) * (n/(n-1))
    std1, std2 = bar1.std(), bar2.std()
    n1, n2 = len(bar1), len(bar2)
    se1, se2 = std1/np.sqrt(n1), std2/np.sqrt(n2)
    sed = np.sqrt(se1**2.0 + se2**2.0)
    # print(std1, std2, n1, n2, se1, se2, sed)
    # print(std)

    plt.errorbar([c1,c2], [bar1.mean(),bar2.mean()], yerr=[std, std], fmt="o", color="black")
    plt.show()
    plt.rcParams['figure.facecolor'] = "white"

"""##graphs for anova test after melt"""

def anovagraphs(dfs_anova, col_v='common', x_val="congruent", value='value'):
    sn.catplot(kind='bar',
           data=dfs_anova,
           x=x_val, hue=col_v, y=value)
    sn.catplot(kind='box',
           data=dfs_anova,
           x=x_val, hue=col_v, y=value)
    sn.catplot(kind='point',
           data=dfs_anova,
            x=x_val, hue=col_v, y=value)
    sn.catplot(kind='strip',
           data=dfs_anova,
            x=x_val, col=col_v, y=value, hue='participant')
    sn.catplot(kind='point',
           data=dfs_anova,
            x=x_val, col=col_v, y=value, hue='participant')

"""# Stroop

## accuracy stroop
"""

def accuracyStroop(dfs, block):
  accuracy_stroop = dfs[(dfs['stroop_anser.corr'] == true) & (dfs['experiment'] != 0)].pivot_table( 
    index=["participant"], columns=[congruent], values=[], aggfunc=[len])
  accuracy_stroop.columns = accuracy_stroop.columns.droplevel(0)
  a = accuracy_stroop[0.0]
  b = accuracy_stroop[1.0]
  print('0: ', a.mean(), a.std())
  print('1: ', b.mean(), b.std())
  res = stat()
  res.ttest(accuracy_stroop, res=[0, 1], test_type=3)
  print(res.summary)
  kdeplot("incongruent", "congrunet", a, b, "stroop accuracy: block " + str(block))
  barplot("incongruent", "congrunet", a, b, "stroop accuracy: block " + str(block))
  return accuracy_stroop

"""## time stroop"""

def timeStroop(dfs, block):
  time_stroop = dfs[(dfs['stroop_anser.corr'] == true) & (dfs['experiment'] != 0)].pivot_table( 
    index=["participant"], columns=[congruent], values=['stroop_anser.rt'], aggfunc=[np.mean])
  time_stroop.columns = time_stroop.columns.droplevel(0)
  time_stroop.columns = time_stroop.columns.droplevel(0)
  a = time_stroop[0.0]
  b = time_stroop[1.0]
  print('0: ', a.mean(), a.std())
  print('1: ', b.mean(), b.std())
  res = stat()
  res.ttest(time_stroop, res=[0, 1], test_type=3)
  print(res.summary)

  kdeplot("incongruent", "congrunet", a, b, "stroop time: block " + str(block))
  barplot("incongruent", "congrunet", a, b, "stroop time: block " + str(block))
  return time_stroop

dfs1_accuracy_stroop = accuracyStroop(dfs1, 1)
dfs1_time_stroop = timeStroop(dfs1, 1)


print("accuracy: expected for 36: 0 = 94, 1 = 89")
display(dfs1_accuracy_stroop.loc[36.0])

print("time: expected for 36: 0 = 0.765, 1 = 0.750")
display(dfs1_time_stroop.loc[36.0])

dfs2_1_accuracy_stroop = accuracyStroop(dfs2_1, 2.1)
dfs2_1_time_stroop = timeStroop(dfs2_1, 2.1)
# dfs2_1_accuracy_stroop

dfs2_2_accuracy_stroop = accuracyStroop(dfs2_2, 2.2)
dfs2_2_time_stroop = timeStroop(dfs2_2, 2.2)

display(dfs2_2[dfs2_2['participant'] == 36])
print("accuracy: expected for 36: 0 = 87, 1 = 88")
display(dfs2_2_accuracy_stroop.loc[36.0])

print("time: expected for 36: 0 = 0.712, 1 = 0.651")
display(dfs2_2_time_stroop.loc[36.0])

dfs_all = dfs1.append(dfs2_1, ignore_index=True)
dfs_all = dfs_all.append(dfs2_2, ignore_index=True)
dfs1_accuracy_stroop = accuracyStroop(dfs_all, 'all')
dfs1_time_stroop = timeStroop(dfs_all, 'all')

"""## error per participant"""

num_std_dist = 3

all_test = pd.concat([dfs1_accuracy_stroop, dfs2_1_accuracy_stroop, dfs2_2_accuracy_stroop], axis=1, ignore_index=True,)
stroop_result_all = all_test.sum(axis=1) / 3
mean_result, std_result = stroop_result_all.mean(), stroop_result_all.std()
print("mean: " + str(mean_result) + " std: " + str(std_result))
stroop_result_all.plot.bar(x='participant')
plt.hlines(mean_result - num_std_dist*std_result, 0, 100, color='black')
plt.show()


print("all test : for 36 : 94, 89, 90, 90, 88, 87" )
display(all_test.loc[36.0])

print("stroop_result_all : for 36: sum 179.3" )
display(stroop_result_all.loc[36.0])


print("stroop_result_all : for 36: mean 2.422" )
display(all_test.loc[36.0].std())


stroop_result_1 = dfs1_accuracy_stroop.sum(axis=1)
mean_result, std_result = stroop_result_1.mean(), stroop_result_1.std()
print("mean: " + str(mean_result) + " std: " + str(std_result))
stroop_result_1.plot.bar(x='participant',)
plt.hlines(mean_result - num_std_dist*std_result, 0, 100, color='black')
plt.show()

stroop_result_2_1 = dfs2_1_accuracy_stroop.sum(axis=1)
mean_result, std_result = stroop_result_2_1.mean(), stroop_result_2_1.std()
print("mean: " + str(mean_result) + " std: " + str(std_result))
stroop_result_2_1.plot.bar(x='participant',)
plt.hlines(mean_result - num_std_dist*std_result, 0, 100, color='black')
plt.show()

stroop_result_2_2 = dfs2_2_accuracy_stroop.sum(axis=1)
mean_result, std_result = stroop_result_2_2.mean(), stroop_result_2_2.std()
print("mean: " + str(mean_result) + " std: " + str(std_result))
stroop_result_2_2.plot.bar(x='participant',)
plt.hlines(mean_result - num_std_dist*std_result, 0, 100, color='black')
plt.show()



"""# show widsp """

display(dfs2_2[dfs2_2['participant'] == 30]["true_shaps"].value_counts())
display(dfs2_1[dfs2_1['participant'] == 30]["true_shaps"].value_counts())
display(dfs1[dfs1['participant'] == 30]["true_shaps"].value_counts())



# dfs2_2.loc[(dfs2_2["true_shaps"] == '10.jpg') | (dfs2_2["true_shaps"] == '8.jpg') |
#        (dfs2_2["true_shaps"] == '14.jpg') |  (dfs2_2["true_shaps"] == '12.jpg') 
#        | (dfs2_2["true_shaps"] == '9.jpg') | (dfs2_2["true_shaps"] == '13.jpg')
#        | (dfs2_2["true_shaps"] == '11.jpg') ,'common'] = 'widespread'
# dfs2_2.loc[(dfs2_2["true_shaps"] == '7.jpg')  | (dfs2_2["true_shaps"] == '5.jpg') 
#        | (dfs2_2["true_shaps"] == '2.jpg') | (dfs2_2["true_shaps"] == '1.jpg')
#        | (dfs2_2["true_shaps"] == '4.jpg') |  (dfs2_2["true_shaps"] == '6.jpg'), 'common'] = 'infrequent'
# dfs2_2.loc[(dfs2_2["true_shaps"] == '3.jpg'), 'common'] = 0



"""# cleanup data - without stroop error and without trail"""

dfs1 = dfs1[(dfs1['stroop_anser.corr'] == true) & (dfs1['experiment'] != 0)]
dfs2_1 = dfs2_1[(dfs2_1['stroop_anser.corr'] == true) & (dfs2_1['experiment'] != 0)]
dfs2_2 = dfs2_2[(dfs2_2['stroop_anser.corr'] == true) & (dfs2_2['experiment'] != 0) & (dfs2_2['common'] != 0)]
display(dfs2_2[dfs2_2[participant] == 36])

dfs1_accurcy1 = dfs1.pivot_table(index=["participant"], 
                                columns=["test_certainty_anser_currect"], values=[],
                                aggfunc=[len])
dfs2_accurcy1 = dfs2.pivot_table(index=["participant"], 
                                columns=["test_uncertainty_anser_currect"], values=[],
                                aggfunc=[len])
x = (dfs1_accurcy1 + dfs2_accurcy1)
display(x.columns)
display((x[('len', True)] / x[('len', True)] + x[('len',    0)]))
(x[('len', True)] / (x[('len', True)] + x[('len',    0)])).mean()

"""# part 1

## accurcy as function of common
"""

dfs1_accurcy = dfs1[dfs1["test_certainty_anser_currect"] == True].pivot_table(index=["participant"], 
                                columns=[congruent, common, "test_certainty_anser_currect"], values=[],
                                aggfunc=[len])
dfs1_accurcy.columns = dfs1_accurcy.columns.droplevel([0, 3])


print("for 36: infre, 0 : 8,  infre, 1 : 11, wides, 0: 54, wides, 1 : 47")
display(dfs1_accurcy.loc[36.0])
print("for 36: mean: 30 , std: 23.87")
print("mean: " + str(dfs1_accurcy.loc[36.0].mean()), "std: " + str(dfs1_accurcy.loc[36.0].std()))
x = dfs1[dfs1["test_certainty_anser_currect"] == True].pivot_table(index=["participant"], 
                                columns=["test_certainty_anser_currect"], values=[],
                                aggfunc=[len])


dfs1_accurcy_mean = pd.DataFrame({"mean": dfs1_accurcy.mean(), "std": dfs1_accurcy.std()})
sn.catplot(kind='bar', data=dfs1_accurcy.mean().reset_index(),
           col="common", x='congruent', y=0)
sn.catplot(kind='bar', data=dfs1_accurcy.std().reset_index(),
           col="common", x='congruent', y=0)
# dfs1_accurcy_mean["mean"].plot.bar(title="mean", hue='common')
# plt.show()
# dfs1_accurcy_mean["std"].plot.bar(title="std")
# plt.show()
dfs1_accurcy_mean

"""## time as function of common """

dfs1_time = dfs1[(dfs1["test_certainty_anser_currect"] == True)].pivot_table(index=["participant"], 
                                columns=[congruent, common], values=["test_certainty_anser_time"],
                                aggfunc=[np.mean])
dfs1_time.columns = dfs1_time.columns.droplevel(level=[0,1])

display(dfs1_time.std())
dfs1_time_mean = pd.DataFrame({"mean": dfs1_time.mean(), "std": dfs1_time.std()})
sn.catplot(kind='bar', data=dfs1_time.mean().reset_index(),
           col="common", x='congruent', y=0)
sn.catplot(kind='bar', data=dfs1_time.std().reset_index(),
           col="common", x='congruent', y=0)
plt.show()

time_for_infreq_dfs1 = dfs1_time.xs("infrequent", level=common, axis=1).reset_index(drop=True)
sn.catplot(kind='bar', data=time_for_infreq_dfs1.mean().reset_index(),
            x='congruent', y=0)
plt.title("time_for_infreq_dfs1")
plt.show();
display(ttest_rel(time_for_infreq_dfs1[0.0], time_for_infreq_dfs1[1.0]))

time_for_widespread_dfs1 = dfs1_time.xs("widespread", level=common, axis=1).reset_index(drop=True)
display((time_for_widespread_dfs1[0.0] - time_for_widespread_dfs1[1.0]).mean())
sn.catplot(kind='bar', data=time_for_widespread_dfs1.mean().reset_index(),
            x='congruent', y=0)
plt.title("time_for_widespread_dfs1")
plt.show();
display(ttest_rel(time_for_widespread_dfs1[0.0], time_for_widespread_dfs1[1.0]))

time_for_dfs1 = dfs1[dfs1["test_certainty_anser_currect"] == True].pivot_table(index=["participant"], 
                                columns=[congruent], values=["test_certainty_mouse.time"],
                                aggfunc=[np.mean]).reset_index(drop=True)
time_for_dfs1.columns = time_for_dfs1.columns.droplevel(level=[0,1])
sn.catplot(kind='bar', data=time_for_dfs1.mean().reset_index(),
            x='congruent', y=0)
plt.title('time_for_dfs1')
plt.show();                    
display(ttest_rel(time_for_dfs1[0.0], time_for_dfs1[1.0]))

display(dfs1[(dfs1["test_certainty_anser_currect"] == True) & (dfs1[participant] == 36) & (dfs1["test_certainty_mouse.time"] != 0)])

def create_se_on_graph(x, se):
  for j, i in enumerate(x.facet_axis(0,0).patches):
    print(i)
    xpoint = [i.get_x() + (i.get_width()/2), i.get_x() + (i.get_width()/2)]
    ypoint = [i.get_height()-se[j], i.get_height()+se[j]]
    plt.plot(xpoint, ypoint,color="black", linewidth=3.0, alpha=0.6)
    print(ypoint)

"""## anova accurcy as function of common & stroop"""

'''
this is a % of accurcy
'''
x = dfs1.pivot_table(index=["participant"], values=[],
                                columns=[congruent, common],
                                aggfunc=[len])
x.columns = x.columns.droplevel(level=[0])
dfs1_accurcy_div = dfs1_accurcy / x

'''
end
'''

dfs1_accurcy_reset = dfs1_accurcy_div.reset_index()
display(dfs1_accurcy_div)
dfs1_accurcy_melt = pd.melt(dfs1_accurcy_reset,id_vars=[("participant", '')],
              value_vars=[(0.0, 'infrequent'),(0.0, 'widespread'),(1.0, 'infrequent'), (1.0, 'widespread')])
dfs1_accurcy_melt = dfs1_accurcy_melt.rename(columns={("participant", ''): "participant"})

dfs1_accurcy_melt=dfs1_accurcy_melt.rename(columns = {'value':'Accuracy (%)'})

dfs1_accurcy_melt.loc[dfs1_accurcy_melt["congruent"] == 0.0,"congruent"] = 'Incongruent'
dfs1_accurcy_melt.loc[dfs1_accurcy_melt["congruent"] == 1.0, "congruent"] = 'Congruent'

dfs1_accurcy_melt.loc[dfs1_accurcy_melt["common"] == 'widespread',"common"] = 'Common'
dfs1_accurcy_melt.loc[dfs1_accurcy_melt["common"] == 'infrequent', "common"] = 'Rare'

dfs1_accurcy_melt=dfs1_accurcy_melt.rename(columns = {congruent:'Congruency'})
dfs1_accurcy_melt=dfs1_accurcy_melt.rename(columns = {common:'Frequency'})

display(pt.rm_anova(dv='Accuracy (%)', within=['Frequency', 'Congruency'], data=dfs1_accurcy_melt, subject="participant",
             detailed=True))
#anovagraphs(dfs1_accurcy_melt, 'Congruency', 'Frequency','accuracy (%)')
x = sn.catplot(kind='bar',
           data=dfs1_accurcy_melt, ci =None,
           x='Frequency', hue="Congruency", y='Accuracy (%)')
se = [0.0141, 0.0135, 0.00844, 0.00758]
create_se_on_graph(x, se)

# dfs1_accurcy

'''
this is a % of accurcy
'''
x = dfs1.pivot_table(index=["participant"], values=[],
                                columns=[congruent, common],
                                aggfunc=[len])
x.columns = x.columns.droplevel(level=[0])
dfs1_accurcy_div = dfs1_accurcy / x


print("for 36: infre, 0: 17, infre, 1: 17, wides, 0:77, wides, 1: 72")
display(x.loc[36.0])
print("for 36: infre, 0: (8/17)=0.47, infre, 1: (11/17)=0.647, wides, 0: (54/77)=0.701, wides, 1: (47/72)=0.652")
display(dfs1_accurcy_div.loc[36.0])


'''
end
'''

dfs1_accurcy_reset = dfs1_accurcy_div.reset_index()
dfs1_accurcy_melt = pd.melt(dfs1_accurcy_reset,id_vars=[("participant", '')],
              value_vars=[(0.0, 'infrequent'),(0.0, 'widespread'),(1.0, 'infrequent'), (1.0, 'widespread')])
dfs1_accurcy_melt = dfs1_accurcy_melt.rename(columns={("participant", ''): "participant"})

dfs1_accurcy_melt=dfs1_accurcy_melt.rename(columns = {'value':'Accuracy (%)'})

dfs1_accurcy_melt.loc[dfs1_accurcy_melt["congruent"] == 0.0,"congruent"] = 'Incongruent'
dfs1_accurcy_melt.loc[dfs1_accurcy_melt["congruent"] == 1.0, "congruent"] = 'Congruent'

dfs1_accurcy_melt.loc[dfs1_accurcy_melt["common"] == 'widespread',"common"] = 'Common'
dfs1_accurcy_melt.loc[dfs1_accurcy_melt["common"] == 'infrequent', "common"] = 'Rare'

dfs1_accurcy_melt=dfs1_accurcy_melt.rename(columns = {congruent:'Congruency'})
dfs1_accurcy_melt=dfs1_accurcy_melt.rename(columns = {common:'Frequency'})

display(pt.rm_anova(dv='Accuracy (%)', within=['Frequency', 'Congruency'], data=dfs1_accurcy_melt, subject="participant",
             detailed=True))
#anovagraphs(dfs1_accurcy_melt, 'Congruency', 'Frequency','accuracy (%)')
x = sn.catplot(kind='bar',
           data=dfs1_accurcy_melt, ci =None,
           x='Frequency', hue="Congruency", y='Accuracy (%)')



# dfs1_accurcy

"""## anova time as function of common & stroop"""

dfs1_time_reset = dfs1_time.reset_index()
x = dfs1_time_reset.copy()
display(x)
dfs1_time_reset = dfs1_time_reset[(dfs1_time_reset["participant"] != '12.0') &
                                  (dfs1_time_reset["participant"] != '13.0') &
                                  (dfs1_time_reset["participant"] != '15.0') ]
display()                                 
dfs1_time_melt = pd.melt(dfs1_time_reset, id_vars=[("participant", '')],
              value_vars=[(0.0, 'infrequent'),(0.0, 'widespread'),(1.0, 'infrequent'), (1.0, 'widespread')])
dfs1_time_melt = dfs1_time_melt.rename(columns={("participant", ''): "participant"})
display(pt.rm_anova(dv='value', within=['common', 'congruent'], data=dfs1_time_melt, subject="participant",
            detailed=True))
dfs1_time_melt=dfs1_time_melt.rename(columns = {'value':'Reaction Time (RT)'})

dfs1_time_melt.loc[dfs1_time_melt["congruent"] == 0.0,"congruent"] = 'Incongruent'
dfs1_time_melt.loc[dfs1_time_melt["congruent"] == 1.0, "congruent"] = 'Congruent'

dfs1_time_melt=dfs1_time_melt.rename(columns = {congruent:'Congruency'})
dfs1_time_melt.loc[dfs1_time_melt["common"] == 'widespread',"common"] = 'Common'
dfs1_time_melt.loc[dfs1_time_melt["common"] == 'infrequent', "common"] = 'Rare'

dfs1_time_melt=dfs1_time_melt.rename(columns = {congruent:'Congruency'})
dfs1_time_melt=dfs1_time_melt.rename(columns = {common:'Frequency'})

x = sn.catplot(kind='bar',
           data=dfs1_time_melt, ci =None,
           x='Frequency', hue="Congruency", y='Reaction Time (RT)')


#x_coords = [p.get_x() + 0.5*p.get_width() for p in x.patches]
#y_coords = [p.get_height() for p in x.patches]
#plt.errorbar(x=x_coords, y=y_coords, yerr=[0.11067, 0.12171, 0.19559, 0.13039], fmt="none", c= "k")
# pt.compute_effsize(dfs1_time_melt, paired=true, eftype='cohen')
se = [0.085448106, 0.087829531, 0.063515238, 0.05674]

create_se_on_graph(x, se)
dfs1_time_reset = x

"""## coraltion with question"""

dfs1_time1 = dfs1[(dfs1["test_certainty_anser_currect"] == True)].pivot_table(index=["participant"], 
                                columns=[], values=["test_certainty_anser_time"],
                                aggfunc=[np.mean])
dfs1_time1.columns = dfs1_time1.columns.droplevel(level=[0])
def corraltion1(data, name):
  path_f = path + "/inhibitory_control&uncertainty&check_score.xlsx"
  QR_score = pd.concat([pd.read_excel(path_f, sheet_name='Sheet0',na_values="").fillna(value = 0)])

  for i in [52, 20, 41]:
    QR_score = QR_score.drop(QR_score[QR_score['participant'] == i].index)
  QR_score = QR_score.sort_values(by=['participant'], ignore_index=True)
  for i in ['depression', 'anxiety', 'stress', 'Anxiety potential',
            'Anxiety control', 'OCI-R', 'IUS',]:
      test_value = i
      a = pd.DataFrame(QR_score[test_value]).merge(data, left_index=True, right_index=True)
      sn.catplot(data=a,
                x=name, y=test_value)
      # sn.regplot(data=a, ci=None,
      #           x='uncertanty', y=test_value)
      plt.show()
      print(i)
      display(pearsonr(a[test_value], a[name]))
corraltion1(dfs1_time1, 'test_certainty_anser_time')

"""##corraltion RT to true & rare to OCI (IUS)"""

x = dfs1[(dfs1["test_certainty_anser_currect"] == True)].pivot_table(index=["participant"], 
                                columns=[common], values=["test_certainty_anser_time"],
                                aggfunc=[np.mean])
x.columns = x.columns.droplevel(level=[0, 1])
x.drop('widespread', inplace=True, axis=1)
corraltion1(x, 'infrequent')

"""## corraltion for RT to true & common """

x = dfs1[(dfs1["test_certainty_anser_currect"] == True)].pivot_table(index=["participant"], 
                                columns=[common], values=["test_certainty_anser_time"],
                                aggfunc=[np.mean])
x.columns = x.columns.droplevel(level=[0, 1])
x.drop('infrequent', inplace=True, axis=1)
corraltion1(x, 'widespread')

"""## corraltion for RT for stroop_0 - stroop_1"""

x = dfs1[(dfs1["test_certainty_anser_currect"] == True)].pivot_table(index=["participant"], 
                                columns=[congruent], values=["test_certainty_anser_time"],
                                aggfunc=[np.mean])
x.columns = x.columns.droplevel(level=[0, 1])
x[0.0] = x[0.0] - x[1.0]
x.drop(1.0, inplace=True, axis=1)
corraltion1(x, 0.0)

"""# part 2.1

## accurracy as function of common - data
"""

display(dfs2_1[dfs2_1["test_uncertainty_anser_currect"] == True].pivot_table(index=["participant"], 
                                                  columns=[common, congruent],values=[], aggfunc=[len]))

"""## error as function of common"""

dfs2_1_accurcy = dfs2_1[dfs2_1["test_uncertainty_anser_currect"] == False].pivot_table(index=["participant"], 
                                columns=[congruent, common, "test_uncertainty_anser_currect"], values=[],
                                aggfunc=[len])
dfs2_1_accurcy.columns = dfs2_1_accurcy.columns.droplevel([0, 3])

print("for 36: infre, 0 : 6,  infre, 1 : 6, wides, 0: 51, wides, 1 : 45")
display(dfs2_1_accurcy.loc[36.0])
print("for 36: mean: 27 , std: 24.37")
print("mean: " + str(dfs2_1_accurcy.loc[36.0].mean()), "std: " + str(dfs2_1_accurcy.loc[36.0].std()))

display(pd.DataFrame({"mean":dfs2_1_accurcy.mean(), "std": dfs2_1_accurcy.std()}))
sn.catplot(kind='bar', data=dfs2_1_accurcy.mean().reset_index(),
           col="common", x='congruent', y=0)
sn.catplot(kind='bar', data=dfs2_1_accurcy.std().reset_index(),
           col="common", x='congruent', y=0)

"""## time as fonction of common & stroop"""

dfs2_1_time = (dfs2_1[dfs2_1["test_uncertainty_anser_currect"] == True]).pivot_table(index=["participant"], 
                                columns=[congruent, common], values=["test_mouse_anser_rual.time"],
                                aggfunc=[np.mean])
dfs2_1_time.columns = dfs2_1_time.columns.droplevel(level=[0,1])


print("for 36: infre, 0 : 2.02,  infre, 1 : 1.81, wides, 0: 1.97, wides, 1 : 2.00")
display(dfs2_1_time.loc[36.0])


dfs2_1_time_mean = pd.DataFrame({"mean": dfs2_1_time.mean(), "std": dfs2_1_time.std()})
sn.catplot(kind='bar', data=dfs2_1_time.mean().reset_index(),
           col="common", x='congruent', y=0)
sn.catplot(kind='bar', data=dfs2_1_time.std().reset_index(),
           col="common", x='congruent', y=0)
dfs2_1_time_mean

"""##anova kind of error as function of the currect answer"""

dfs2_1[infrequnce_error] = np.select([(dfs2_1[experiment] == "uncertainty") &
                                        (dfs2_1[click_name] == "three_right")], ["infreq_answer"], default="wides_answer")
dfs2_1.loc[dfs2_1["test_uncertainty_anser_currect"] == True, infrequnce_error] = "true_answer"
display(dfs2_1)
#dfs2_1[infrequnce_error] = np.where(dfs2_1["test_uncertainty_anser_currect"] == True, "true_answer", dfs2_1[infrequnce_error])
dfs2_1_accurcy = dfs2_1.pivot_table(index=["participant"], 
                                columns=[common, infrequnce_error], values=[],
                                aggfunc=[len])
dfs2_1_accurcy.columns = dfs2_1_accurcy.columns.droplevel(level=[0])
'''
this is a % of error
'''
# x = dfs2_1.pivot_table(index=["participant"], values=[],
#                                 columns=[common],
#                                 aggfunc=[len]).fillna(0)
# x.columns = x.columns.droplevel(level=[0])
# dfs1_accurcy_div = dfs2_1_accurcy / x 
# display(x)
display(dfs2_1_accurcy)
'''
end
'''
dfs2_1_accurcy = dfs2_1_accurcy.reset_index()
dfs2_1_accurcy_melt = pd.melt(dfs2_1_accurcy, id_vars=[dfs2_1_accurcy.columns[0]],
              value_vars=[dfs2_1_accurcy.columns[i] for i in range(1,len(dfs2_1_accurcy.columns))])
display(dfs2_1_accurcy_melt)
dfs2_1_accurcy_melt = dfs2_1_accurcy_melt.rename(columns={("participant", ''): "participant"})
display(pt.rm_anova(dv='value', within=['common', 'infrequnce_error'], data=dfs2_1_accurcy_melt, subject="participant",
             detailed=True))
anovagraphs(dfs2_1_accurcy_melt, x_val=infrequnce_error)

"""## anova 3-way kind of error as function of currect answer, common & stroop """

dfs2_1_accurcy_3_way = dfs2_1.pivot_table(
                                index=["participant"], 
                                columns=[congruent, common, infrequnce_error], values=[],
                                aggfunc=[len], fill_value=0).fillna(0)
dfs2_1_accurcy_3_way.columns = dfs2_1_accurcy_3_way.columns.droplevel(level=[0])

mean = dfs2_1.pivot_table(index=["participant"],columns=infrequnce_error,aggfunc=[len], values=[])
mean.columns = mean.columns.droplevel(level=[0])
print(mean.mean())
print(mean.std())
display(ttest_1samp(mean, 6.6))
res = stat()
res.ttest(df=mean, test_type=1, mu=6.6, res='true_answer')
print(res.summary)



infreq_answer_zero_row = pd.DataFrame([{participant:i,congruent:c, common:'infrequent',
                                        infrequnce_error:'infreq_answer', 'value':0} 
                                       for i in dfs2_1_accurcy_3_way.index for c in [0, 1]])
dfs2_1_accurcy_3_way = dfs2_1_accurcy_3_way.reset_index()

len_of_answer_common = dfs2_1.pivot_table(index=["participant"], 
                                columns=[congruent, common], values=[],
                                aggfunc=[len], fill_value=0).fillna(0)
len_of_answer_common.columns = len_of_answer_common.columns.droplevel(level=[0])

x = dfs2_1_accurcy_3_way
dfs2_1_accurcy_3_way_copy_without_precent = dfs2_1_accurcy_3_way.copy()
s1 = pd.DataFrame(x[(0, 'widespread', 'true_answer')])
x[(0, 'widespread', 'true_answer')]  = (s1.values / pd.DataFrame(len_of_answer_common[(0, "widespread")]).values)
s2 = pd.DataFrame(x[(1, 'widespread', 'true_answer')])
x[(1, 'widespread',   'true_answer')] = (s2.values / pd.DataFrame(len_of_answer_common[(1, "widespread")]).values)
s1 = pd.DataFrame(x[(0, 'infrequent', 'true_answer')])
x[(0, 'infrequent', 'true_answer')]  = (s1.values / pd.DataFrame(len_of_answer_common[(0, "infrequent")]).values)
s2 =pd.DataFrame(x[(1, 'infrequent', 'true_answer')])
x[(1, 'infrequent',   'true_answer')] = (s2.values / pd.DataFrame(len_of_answer_common[(1, "infrequent")]).values)

display((dfs2_1_accurcy_3_way.xs(('infreq_answer'), level=infrequnce_error, axis=1)))
display((dfs2_1_accurcy_3_way.xs(('wides_answer'), level=infrequnce_error, axis=1)))

dfs2_1_accurcy_melt_3_way = pd.melt(dfs2_1_accurcy_3_way, id_vars=[dfs2_1_accurcy_3_way.columns[0]],
              value_vars=[dfs2_1_accurcy_3_way.columns[i] for i in range(1, len(dfs2_1_accurcy_3_way.columns))])
dfs2_1_accurcy_melt_3_way = dfs2_1_accurcy_melt_3_way.rename(columns={("participant", '', ''): "participant"})
dfs2_1_accurcy_melt_3_way = pd.concat([dfs2_1_accurcy_melt_3_way , infreq_answer_zero_row], ignore_index=True)
from statsmodels.stats.anova import AnovaRM

"""## step 1: anova for true answer : common & conguent (%)"""

dfs2_1_accurcy_melt_3_way_copy = dfs2_1_accurcy_melt_3_way.copy()
# dfs2_1_accurcy_melt_3_way_copy = dfs2_1_accurcy_melt_3_way_copy[(dfs2_1_accurcy_melt_3_way_copy['participant']!=39)]

# dfs2_1_accurcy_melt_3_way_copy.loc[dfs2_1_accurcy_melt_3_way_copy[common]== "widespread", "value"] = dfs2_1_accurcy_melt_3_way_copy["value"]/ len_of_answer_common["widespread"]
# dfs2_1_accurcy_melt_3_way_copy.loc[dfs2_1_accurcy_melt_3_way_copy[common]== "infrequent", "value"] = dfs2_1_accurcy_melt_3_way_copy["value"]/ len_of_answer_common["infrequent"]
display()
aov = AnovaRM(
    dfs2_1_accurcy_melt_3_way_copy[(dfs2_1_accurcy_melt_3_way_copy[infrequnce_error] == 'true_answer')],
    depvar='value',
    subject="participant",
    within=['congruent', 'common']
    ).fit()
print(aov)
display(pt.rm_anova(dv='value', within=['common', 'congruent'],
  data=dfs2_1_accurcy_melt_3_way_copy[(dfs2_1_accurcy_melt_3_way_copy[infrequnce_error] == 'true_answer')],
  subject="participant", detailed=True))
x=dfs2_1_accurcy_melt_3_way_copy.rename(columns = {'value':'Accuracy (%)'})
x=x.rename(columns = {infrequnce_error:'answer'})
x.loc[x["congruent"] == 0.0,"congruent"] = 'Incongruent'
x.loc[x["congruent"] == 1.0, "congruent"] = 'Congruent'
x=x.rename(columns = {congruent:'Congruency'})
x.loc[x["answer"] == 'true_answer',"answer"] = 'true answer'

x.loc[x["common"] == 'widespread',"common"] = 'Common'
x.loc[x["common"] == 'infrequent', "common"] = 'Rare'

x=x.rename(columns = {common:'Frequency'})

z = sn.catplot(kind='bar', 
           data=x[x['answer'] == 'true answer'], ci=None,
           x='Frequency', col='answer', hue='Congruency', y='Accuracy (%)')

se = [0.027451017, 0.030261238, 0.029161682, 0.02223]
create_se_on_graph(z, se)



"""## steps 2: t-test for widsp & true answer : conguent (%)"""

dfs_2_1_true_answer_common_a = dfs2_1_accurcy_melt_3_way_copy[(dfs2_1_accurcy_melt_3_way_copy[infrequnce_error] == 'true_answer') &
                              (dfs2_1_accurcy_melt_3_way_copy[common] == 'widespread')]
display(dfs_2_1_true_answer_common_a)
aov = AnovaRM(
    dfs_2_1_true_answer_common_a,
    depvar='value',
    subject="participant",
    within=['congruent']
    ).fit()
print(aov)
display(pt.pairwise_tests(data=dfs_2_1_true_answer_common_a, dv='value', 
                         within=['congruent'], subject="participant", effsize='eta-square'))


dfs_2_1_true_answer_common = dfs_2_1_true_answer_common_a.pivot_table(index=["participant"], 
                                columns=[congruent],values="value").fillna(0) 
display(dfs_2_1_true_answer_common.columns)

                              
res = stat()

res.ttest(dfs_2_1_true_answer_common, res=[0,1], test_type=3)                
print(res.summary)
z = sn.catplot(kind='bar', ci=None,
           data= dfs_2_1_true_answer_common)

se = [0.007844493, 0.00784]
create_se_on_graph(z, se)

"""## step 3: anova for error - widsp : kind of error & stroop"""

dfs2_1_accurcy_melt_3_way_copy = dfs2_1_accurcy_melt_3_way.copy()
x_2_1 = dfs2_1_accurcy_melt_3_way_copy[(dfs2_1_accurcy_melt_3_way_copy['common'] == "widespread") &
                              (dfs2_1_accurcy_melt_3_way_copy[infrequnce_error] != "true_answer")]
                            
aov = AnovaRM(
    x_2_1,
    depvar='value',
    subject="participant",
    within=['congruent', infrequnce_error]
    ).fit()

display(pt.rm_anova(dv='value', within=['congruent', infrequnce_error],
  data=x_2_1,
  subject="participant", detailed=True))

x=x_2_1.rename(columns = {'value':'Number of Error'})
x=x.rename(columns = {infrequnce_error:'kind of error'})
x.loc[x['kind of error'] == 'infreq_answer',"kind of error"] = 'Uncertainty'
x.loc[x['kind of error'] == 'wides_answer',"kind of error"] = 'Perceptual'
x.loc[x["congruent"] == 0.0,"congruent"] = 'Incongruent'
x.loc[x["congruent"] == 1.0, "congruent"] = 'Congruent' 
x=x.rename(columns = {congruent:'Congruency'})
x=x.rename(columns = {"kind of error":'Error Type'})
print(aov)
z = sn.catplot(kind='bar', ci=None,
           data=x,
           
           col='common', x='Error Type', hue='Congruency', y='Number of Error')
se = [1.765422978,1.820661389, 1.487190227, 2.08179]
create_se_on_graph(z, se)

"""##step 4: ttest for error - widsp & """

aov = AnovaRM(
    x_2_1[(x_2_1[infrequnce_error] == 'infreq_answer')],
    depvar='value',
    subject="participant",
    within=['congruent']
    ).fit()
print(aov)
# display(x_2_1[(x_2_1[infrequnce_error] == 'infreq_answer')])
display(pt.pairwise_tests(data=x_2_1[(x_2_1[infrequnce_error] == 'infreq_answer')], dv='value', 
                         within=['congruent'], subject="participant", effsize='eta-square'))
x1 = x_2_1[(x_2_1[infrequnce_error] == 'infreq_answer')].pivot_table(index=["participant"], 
                                columns=[congruent],values="value").fillna(0) 

res = stat()
res.ttest(x1, res=[0,1], test_type=3)
display((x1[0]-x1[1]).mean()) 
# display((x1[0]-x1[1]).std() / np.sqrt(np.size(x1[0])))  
display((x1[0]-x1[1]).std())  
display(x1)          
print(res.summary)
x1 = pd.melt(x1)
display(x1)
z = sn.catplot(kind='bar', 
           data=x1,
           ci=None,
           x='congruent', y='value')
se = [0.412,0.412]
create_se_on_graph(z, se)

"""## % of (error say no image) / (say no image) """

p = dfs2_1_accurcy_3_way_copy_without_precent
true_answer_0 = pd.DataFrame(p[(0, 'infrequent', 'true_answer')])
error_0 = pd.DataFrame(p[(0, 'widespread', 'infreq_answer')])
stroop_0 = (error_0.values / (error_0.values + true_answer_0.values)).reshape(1, -1).flatten()

true_answer_1 = pd.DataFrame(p[(1, 'infrequent', 'true_answer')])
error_1 = pd.DataFrame(p[(1, 'widespread', 'infreq_answer')])
stroop_1 = (error_1.values / (error_1.values + true_answer_1.values)).reshape(1, -1).flatten()

present_of_uncertanty = pd.DataFrame([stroop_0, stroop_1]).T
present_of_uncertanty

uncertanty_all = pd.DataFrame(((error_1.values+error_0.values) / (error_1.values + true_answer_1.values +
                                                error_0.values + true_answer_0.values)).reshape(1, -1).flatten())
#uncertanty_all = pd.DataFrame((error_1.values + true_answer_1.values + error_0.values + true_answer_0.values).reshape(1, -1).flatten())
uncertanty_all=uncertanty_all.rename(columns = {0:'uncertanty'})
uncertanty_all = pd.DataFrame(p[participant]).merge(uncertanty_all, left_index=True, right_index=True)
res = stat()
res.ttest(present_of_uncertanty, res=[0,1], test_type=3)  
display(ttest_1samp(a=stroop_0-stroop_1, popmean=0) )           
print(res.summary)
sn.catplot(kind='bar',
           data=present_of_uncertanty)

"""corraltion

## coraltion with question
"""

def corraltion(data, name):
  path_f = path + "/inhibitory_control&uncertainty&check_score.xlsx"
  QR_score = pd.concat([pd.read_excel(path_f, sheet_name='Sheet0',na_values="").fillna(value = 0)])

  for i in [52, 20, 41]:
    QR_score = QR_score.drop(QR_score[QR_score['participant'] == i].index)
  QR_score = QR_score.sort_values(by=['participant'], ignore_index=True)
  # for i in ['accumulation', 'Obsessiveness', 'checking', 'neutralize',
  #           'order', 'depression', 'anxiety', 'stress', 'Anxiety potential',
  #           'Anxiety control', 'OCI-R', 'IUS', 'washing']:
  for i in ['IUS']:
      display(data)
      test_value = i
      a = pd.DataFrame(QR_score[test_value]).merge(data, left_index=True, right_index=True)
      # a=a.rename(columns = {'depression':'Depression'})
      # a=a.rename(columns = {'infreq_answer':'Accepting Uncertainty'})
      # name = 'Accepting Uncertainty'
      # test_value = 'Depression'
      sn.catplot(data=a,
                 x=name, y=test_value)
      sn.lmplot(data=a, ci = None,
                 x=name, y=test_value)
      #sn.regplot(data=a, ci=None,
      #          x='Uncertainty Propagation', y=test_value)
    
      plt.show()
      print(i)
      display(pearsonr(a[test_value], a[name]))
# corraltion(uncertanty_all, 'uncertanty' )   
# display(uncertanty_all)

"""## corraltion - for time - say no image """

dfs2_1_copy = dfs2_1.copy()
dfs2_1_copy[infrequnce_error] = np.select([(dfs2_1_copy[experiment] == "uncertainty") &
                                        (dfs2_1_copy[click_name] == "three_right")], ["infreq_answer"], default="wides_answer")
########### for test only error uncertanty, take the next line
# dfs2_1_copy.loc[dfs2_1["test_uncertainty_anser_currect"] == True, infrequnce_error] = "true_answer"
dfs2_1_time_uncertanty = (dfs2_1_copy.pivot_table(index=["participant"], 
                                columns=[infrequnce_error], values=["test_mouse_anser_rual.time"],
                                aggfunc=[np.mean]))
dfs2_1_time_uncertanty.columns = dfs2_1_time_uncertanty.columns.droplevel(level=[0, 1])
########### for test only error uncertanty, take the next line
# dfs2_1_time_uncertanty.drop('true_answer', inplace=True, axis=1)
dfs2_1_time_uncertanty.drop('wides_answer', inplace=True, axis=1)
dfs2_1_time_uncertanty
corraltion(dfs2_1_time_uncertanty, 'infreq_answer')

"""## corraltion - for time - all kind error"""

dfs2_1_copy[infrequnce_error] = np.select([(dfs2_1_copy[experiment] == "uncertainty") &
                                        (dfs2_1_copy[click_name] == "three_right")], ["error"], default="error")
dfs2_1_copy.loc[dfs2_1["test_uncertainty_anser_currect"] == True, infrequnce_error] = "true_answer"
dfs2_1_time_uncertanty = (dfs2_1_copy.pivot_table(index=["participant"], 
                                columns=[infrequnce_error], values=["test_mouse_anser_rual.time"],
                                aggfunc=[np.mean]))
dfs2_1_time_uncertanty.columns = dfs2_1_time_uncertanty.columns.droplevel(level=[0, 1])
########### for test only error uncertanty, take the next line
dfs2_1_time_uncertanty.drop('true_answer', inplace=True, axis=1)
dfs2_1_time_uncertanty
corraltion(dfs2_1_time_uncertanty, 'error')

"""## corraltion - accurcy - after inhibtion"""

diffAccurcy = pd.DataFrame(stroop_0 - stroop_1)
diffAccurcy=diffAccurcy.rename(columns = {0:'uncertanty'})
display(diffAccurcy)
corraltion(diffAccurcy, 'uncertanty')

"""# part 2.2

## accuracy as function of common - data
"""

dfs2_2[dfs2_2["test_uncertainty_anser_currect"] == True].pivot_table(index=["participant"], 
                                                  columns=[common, congruent],values=[], aggfunc=[len])

"""## error as function of common"""

dfs2_2_accurcy = dfs2_2[dfs2_2["test_uncertainty_anser_currect"] == False].pivot_table(index=["participant"], 
                                columns=[congruent, common, "test_uncertainty_anser_currect"], values=[],
                                aggfunc=[len])
dfs2_2_accurcy.columns = dfs2_2_accurcy.columns.droplevel([0, 3])


display(dfs2_2_accurcy.loc[36.0])


display(pd.DataFrame({"mean":dfs2_2_accurcy.mean(), "std": dfs2_2_accurcy.std()}))
sn.catplot(kind='bar', data=dfs2_2_accurcy.mean().reset_index(),
           col="common", x='congruent', y=0)
sn.catplot(kind='bar', data=dfs2_2_accurcy.std().reset_index(),
           col="common", x='congruent', y=0)

"""## add infrequnce error to data"""

def add_error_of_infrequnce(file):
    # file[infrequnce_error] = np.select([(file[experiment] == "certainty") &
    #                                     ((file[click_name] == "two_up")
    #                                      | (file[click_name] == "one_up") | (file[click_name] == "three_up") |
    #                                      (file[click_name] == "four_up") | (file[click_name] == "one_down")
    #                                      | (file[click_name] == "two_down") | (file[click_name] == "three_down"))],
    #                                    ["infreq_answer"], default="wides_answer")
    file[infrequnce_error] = np.select([(file[experiment] == "certainty") &
                                        ((file[click_name] == "two_up") | (file[click_name] == "one_down")
                                         | (file[click_name] == "three_down"))],
                                       ["infreq_answer"], default="wides_answer")
    # file = file[(file[stroop_corr] == 1) & (file[infrequnce_error] == True)] \
    #     .pivot_table(index=["participant"], columns=[congruent], values=[], aggfunc=[len])
    return file
dfs2_2 = add_error_of_infrequnce(dfs2_2)
display(dfs2_2)

"""##anova kind of error as function of the currect answer"""

dfs2_2_accurcy = dfs2_2[dfs2_2["test_uncertainty_anser_currect"] == False].pivot_table(index=["participant"], 
                                columns=[common, infrequnce_error], values=[],
                                aggfunc=[len])
dfs2_2_accurcy.columns = dfs2_2_accurcy.columns.droplevel(level=[0])                          
dfs2_2_accurcy = dfs2_2_accurcy.reset_index()
dfs2_2_accurcy_melt = pd.melt(dfs2_2_accurcy, id_vars=[dfs2_2_accurcy.columns[0]],
              value_vars=[dfs2_2_accurcy.columns[i] for i in range(1,len(dfs2_2_accurcy.columns))])
dfs2_2_accurcy_melt = dfs2_2_accurcy_melt.rename(columns={("participant", ''): "participant"})
display(pt.rm_anova(dv='value', within=['common', 'infrequnce_error'], data=dfs2_2_accurcy_melt, subject="participant",
             detailed=True))

anovagraphs(dfs2_2_accurcy_melt, x_val=infrequnce_error)

"""## anova 3-way"""

dfs2_2.loc[dfs2_2["test_uncertainty_anser_currect"] == True, infrequnce_error] = "true_answer"
dfs2_2_accurcy_3_way = dfs2_2.pivot_table(
                                index=["participant"], 
                                columns=[congruent, common, infrequnce_error], values=[],
                                aggfunc=[len], fill_value=0).fillna(0)
u = dfs2_2.pivot_table(
                                index=["participant"], 
                                columns=[common, infrequnce_error], values=[],
                                aggfunc=[len], fill_value=0).fillna(0)

u.columns = u.columns.droplevel(level=[0])
u = u.reset_index()


mean = dfs2_2.pivot_table(index=["participant"],columns=infrequnce_error,aggfunc=[len], values=[])
mean.columns = mean.columns.droplevel(level=[0])
print(mean.mean())
print(mean.std())
display(ttest_1samp(mean, 6.6))
res = stat()
res.ttest(df=mean, test_type=1, mu=7.1, res='true_answer')
print(res.summary)

dfs2_2_accurcy_3_way.columns = dfs2_2_accurcy_3_way.columns.droplevel(level=[0])
dfs2_2_accurcy_3_way = dfs2_2_accurcy_3_way.reset_index()
#display(dfs2_2_accurcy_3_way)
len_of_answer_common = dfs2_2.pivot_table(index=["participant"], 
                                columns=[congruent, common], values=[],
                                aggfunc=[len], fill_value=0).fillna(0)
len_of_answer_common.columns = len_of_answer_common.columns.droplevel(level=[0])


s1 = pd.DataFrame(u[('widespread', 'true_answer')])
u[('widespread', 'true_answer')]  = (s1.values / (pd.DataFrame(len_of_answer_common[(0, "widespread")]).values + pd.DataFrame(len_of_answer_common[(1, "widespread")]).values))
s1 = pd.DataFrame(u[('infrequent', 'true_answer')])
u[('infrequent', 'true_answer')]  = (s1.values / (pd.DataFrame(len_of_answer_common[(0, "infrequent")]).values + pd.DataFrame(len_of_answer_common[(1, "widespread")]).values))
print(u.xs(('true_answer'), level=infrequnce_error, axis=1).mean(axis=0))
print(u.xs(('true_answer'), level=infrequnce_error, axis=1).std(axis=0))
display(u.xs(('true_answer'), level=infrequnce_error, axis=1))




dfs2_2_accurcy_3_way_copy_without_precent = dfs2_2_accurcy_3_way.copy()
x = dfs2_2_accurcy_3_way
s1 = pd.DataFrame(x[(0, 'widespread', 'true_answer')])
x[(0, 'widespread', 'true_answer')]  = (s1.values / pd.DataFrame(len_of_answer_common[(0, "widespread")]).values)
s2 = pd.DataFrame(x[(1, 'widespread', 'true_answer')])
x[(1, 'widespread',   'true_answer')] = (s2.values / pd.DataFrame(len_of_answer_common[(1, "widespread")]).values)
s1 = pd.DataFrame(x[(0, 'infrequent', 'true_answer')])
x[(0, 'infrequent', 'true_answer')]  = (s1.values / pd.DataFrame(len_of_answer_common[(0, "infrequent")]).values)
s2 =pd.DataFrame(x[(1, 'infrequent', 'true_answer')])
x[(1, 'infrequent',   'true_answer')] = (s2.values / pd.DataFrame(len_of_answer_common[(1, "infrequent")]).values)

#display((dfs2_2_accurcy_3_way.xs(('true_answer'), level=infrequnce_error, axis=1)))

#display((dfs2_2_accurcy_3_way.xs(('infreq_answer'), level=infrequnce_error, axis=1)))
#display((dfs2_2_accurcy_3_way.xs(('wides_answer'), level=infrequnce_error, axis=1)))
#display(dfs2_2_accurcy_3_way)

# find mean
#u = dfs2_2_accurcy_3_way.xs(('true_answer'), level=infrequnce_error, axis=1)

# display(u[(1, "infreq_answer")] + u[(0, "infreq_answer")])


dfs2_2_accurcy_melt_3_way = pd.melt(dfs2_2_accurcy_3_way, id_vars=[dfs2_2_accurcy_3_way.columns[0]],
              value_vars=[dfs2_2_accurcy_3_way.columns[i] for i in range(1, len(dfs2_2_accurcy_3_way.columns))])

dfs2_2_accurcy_melt_3_way = dfs2_2_accurcy_melt_3_way.rename(columns={("participant", '', ''): "participant"})
dfs2_2_accurcy_melt_3_way = pd.concat([dfs2_2_accurcy_melt_3_way], ignore_index=True)

# dfs2_2_accurcy_melt_3_way.loc[dfs2_2_accurcy_melt_3_way[common]== "infrequent", "value"] = dfs2_2_accurcy_melt_3_way["value"]/ 20
# dfs2_2_accurcy_melt_3_way.loc[dfs2_2_accurcy_melt_3_way[common]== "widespread", "value"] = dfs2_2_accurcy_melt_3_way["value"]/ 80

# from statsmodels.stats.anova import AnovaRM

aov = AnovaRM(
    # dfs2_2_accurcy_melt_3_way[(dfs2_2_accurcy_melt_3_way['common'] == "widespread")],
    # # dfs2_2_accurcy_melt_3_way[(dfs2_2_accurcy_melt_3_way['common'] == "widespread")
    # # & (dfs2_2_accurcy_melt_3_way[infrequnce_error] == 'true_answer')],
    dfs2_2_accurcy_melt_3_way[(dfs2_2_accurcy_melt_3_way[infrequnce_error] == 'true_answer')], 
    depvar='value',
    subject="participant",
    # within=['common', 'congruent', infrequnce_error], 
    # within=['congruent', infrequnce_error],
    within=['congruent', 'common']
    ).fit()
print(aov)
# # 
# # display(dfs2_1_accurcy_melt_3_way[(dfs2_1_accurcy_melt_3_way['common'] == "widespread")
# #     & (dfs2_1_accurcy_melt_3_way[infrequnce_error] == 'true_answer')])
#sn.catplot(kind='bar',
           #data= dfs2_2_accurcy_melt_3_way[
                                          #  (dfs2_2_accurcy_melt_3_way['common'] == "widespread")
            #(dfs2_2_accurcy_melt_3_way[infrequnce_error] == 'true_answer')], units='participant',
            #x='congruent', y='value')
#sn.catplot(kind='bar',
         #  data=dfs2_2_accurcy_melt_3_way,
         #  x='common', col=infrequnce_error, hue='congruent', y='value')
#sn.catplot(kind='box',
 #          data=dfs2_2_accurcy_melt_3_way,
 #         x='common', col='congruent',hue=infrequnce_error, y='value')
#sn.catplot(kind='point',
 #          data=dfs2_2_accurcy_melt_3_way,
 #            x='common', col='congruent', hue=infrequnce_error, y='value')
#sn.catplot(kind='strip',
 #          data=dfs2_2_accurcy_melt_3_way,
 #           x='common', col='congruent', y='value', hue=infrequnce_error)
#sn.catplot(kind='box',
#           data=dfs2_2_accurcy_melt_3_way,
#            x='congruent', col="common", y='value', hue=infrequnce_error)

"""## step 1: anova for true answer : common & conguent (%)"""

dfs2_2_accurcy_melt_3_way_copy = dfs2_2_accurcy_melt_3_way.copy()
display(dfs2_2.pivot_table(index=["participant"], 
                                columns=[common], values=[],
                                aggfunc=[len], fill_value=0).fillna(0))
#dfs2_2_accurcy_melt_3_way_copy.loc[dfs2_2_accurcy_melt_3_way_copy[common]== "widespread", "value"] = dfs2_2_accurcy_melt_3_way_copy["value"]/ 20

#dfs2_2_accurcy_melt_3_way_copy.loc[dfs2_2_accurcy_melt_3_way_copy[common]== "infrequent", "value"] = dfs2_2_accurcy_melt_3_way_copy["value"]/ len(dfs2_2_accurcy_melt_3_way_copy[common]== "infrequent")
aov = AnovaRM(
    dfs2_2_accurcy_melt_3_way_copy[(dfs2_2_accurcy_melt_3_way_copy[infrequnce_error] == 'true_answer')],
    depvar='value',
    subject="participant",
    within=['congruent', 'common']
    ).fit()
print(aov)
display(pt.rm_anova(dv='value', within=['congruent', 'common'],
  data=dfs2_2_accurcy_melt_3_way_copy[(dfs2_2_accurcy_melt_3_way_copy[infrequnce_error] == 'true_answer')],
  subject="participant", detailed=True))

x=dfs2_2_accurcy_melt_3_way_copy.rename(columns = {'value':'Accuracy (%)'})
x=x.rename(columns = {infrequnce_error:'answer'})
x.loc[x["answer"] == 'true_answer',"answer"] = 'true answer'
x.loc[x["congruent"] == 0.0,"congruent"] = 'Incongruent'
x.loc[x["congruent"] == 1.0, "congruent"] = 'Congruent'
x=x.rename(columns = {congruent:'Congruency'})

x.loc[x["common"] == 'widespread',"common"] = 'Common'
x.loc[x["common"] == 'infrequent', "common"] = 'Rare'

x=x.rename(columns = {common:'Frequency'})


z = sn.catplot(kind='bar',
           data= x[x['answer'] == 'true answer'], ci=None,
           x='Frequency', col='answer', hue='Congruency', y='Accuracy (%)')

se = [0.0203, 0.017,0.014, 0.011]
create_se_on_graph(z, se)



"""## steps 2: t-test for widsp & true answer : conguent (%)"""

dfs_2_2_true_answer_common_a = dfs2_2_accurcy_melt_3_way_copy[(dfs2_2_accurcy_melt_3_way_copy[infrequnce_error] == 'true_answer') &
                              (dfs2_2_accurcy_melt_3_way_copy[common] == 'widespread')]
                              

aov = AnovaRM(
    dfs_2_2_true_answer_common_a,
    depvar='value',
    subject="participant",
    within=['congruent']
    ).fit()
print(aov)


dfs_2_2_true_answer_common = dfs_2_2_true_answer_common_a.pivot_table(index=["participant"], 
                                columns=[congruent],values="value").fillna(0)                               
res = stat()
res.ttest(dfs_2_2_true_answer_common, res=[0,1], test_type=3)                
print(res.summary)
z = sn.catplot(kind='bar', ci=None,
           data= dfs_2_2_true_answer_common)

se = [0.00784, 0.00784]
create_se_on_graph(z, se)

"""## step 3: anova for error - widsp : kind of error & stroop"""

x_2_2 = dfs2_2_accurcy_melt_3_way[(dfs2_2_accurcy_melt_3_way['common'] == "widespread") &
                              (dfs2_2_accurcy_melt_3_way[infrequnce_error] != "true_answer")]
                              #  (dfs2_1_accurcy_melt_3_way['participant'] != 30 )] 
aov = AnovaRM(
    x_2_2,
    depvar='value',
    subject="participant",
    within=['congruent', infrequnce_error]
    ).fit()
print(aov)

display(pt.rm_anova(dv='value', within=['congruent', infrequnce_error],
  data=x_2_2,
  subject="participant", detailed=True))

x=x_2_2.rename(columns = {'value':'Number of Error'})
x=x.rename(columns = {infrequnce_error:'kind of error'})
x.loc[x['kind of error'] == 'infreq_answer',"kind of error"] = 'Rare error'
x.loc[x['kind of error'] == 'wides_answer',"kind of error"] = 'Common error'
x.loc[x["congruent"] == 0.0,"congruent"] = 'incongruent'
x.loc[x["congruent"] == 1.0, "congruent"] = 'congruent'

x.loc[x['kind of error'] == 'infreq_answer',"kind of error"] = 'Uncertainty'
x.loc[x['kind of error'] == 'wides_answer',"kind of error"] = 'Perceptual'
x.loc[x["congruent"] == 0.0,"congruent"] = 'Incongruent'
x.loc[x["congruent"] == 1.0, "congruent"] = 'Congruent' 
x=x.rename(columns = {congruent:'Congruency'})
x=x.rename(columns = {"kind of error":'Error Type'})

z = sn.catplot(kind='bar',
           data=x,
           units='participant', ci=None,
           col='common', x='Error Type', hue='Congruency', y='Number of Error')
se = [1.1, 1.0733, 1.6373, 1.376]
create_se_on_graph(z, se)

"""##step 4: ttest for error - widsp & """

aov = AnovaRM(
    x_2_2[(x_2_2[infrequnce_error] == 'infreq_answer')],
    depvar='value',
    subject="participant",
    within=['congruent']
    ).fit()
print(aov)
x1 = x_2_2[x_2_2[infrequnce_error] == 'infreq_answer'].pivot_table(index=["participant"], 
                                columns=[congruent],values="value").fillna(0) 

res = stat()
res.ttest(x1, res=[0,1], test_type=3)                
print(res.summary)

z = sn.catplot(kind='bar', ci=None,
           data=x_2_2[x_2_2[infrequnce_error] == 'infreq_answer'],
           x='congruent', y='value')
se = [0.4126, 0.4126]
create_se_on_graph(z, se)

p = dfs2_2_accurcy_3_way_copy_without_precent
true_answer_0 = pd.DataFrame(p[(0, 'infrequent', 'true_answer')])
error_0 = pd.DataFrame(p[(0, 'widespread', 'infreq_answer')])
stroop_0 = (error_0.values / (error_0.values + true_answer_0.values)).reshape(1, -1).flatten()

true_answer_1 = pd.DataFrame(p[(1, 'infrequent', 'true_answer')])
error_1 = pd.DataFrame(p[(1, 'widespread', 'infreq_answer')])
stroop_1 = (error_1.values / (error_1.values + true_answer_1.values)).reshape(1, -1).flatten()

present_of_uncertanty = pd.DataFrame([stroop_0, stroop_1]).T
present_of_uncertanty

uncertanty_all = pd.DataFrame(((error_1.values+error_0.values) / (error_1.values + true_answer_1.values +
                                                error_0.values + true_answer_0.values)).reshape(1, -1).flatten())
#uncertanty_all = pd.DataFrame((error_1.values + true_answer_1.values + error_0.values + true_answer_0.values).reshape(1, -1).flatten())
uncertanty_all=uncertanty_all.rename(columns = {0:'rare choose'})
uncertanty_all = pd.DataFrame(p[participant]).merge(uncertanty_all, left_index=True, right_index=True)
res = stat()
res.ttest(present_of_uncertanty, res=[0,1], test_type=3)                
print(res.summary)
sn.catplot(kind='bar',
           data=present_of_uncertanty)

"""##correlation with question"""

path_f = path + "/inhibitory_control&uncertainty&check_score.xlsx"
QR_score = pd.concat([pd.read_excel(path_f, sheet_name='Sheet0',na_values="").fillna(value = 0)])

for i in [52, 20, 41]:
  QR_score = QR_score.drop(QR_score[QR_score['participant'] == i].index)
QR_score = QR_score.sort_values(by=['participant'], ignore_index=True)
for i in ['accumulation', 'Obsessiveness', 'checking', 'neutralize',
          'order', 'depression', 'anxiety', 'stress', 'Anxiety potential',
          'Anxiety control', 'OCI-R', 'IUS', 'washing']:
    test_value = i
    a = pd.DataFrame(QR_score[test_value]).merge(uncertanty_all, left_index=True, right_index=True)
    sn.catplot(data=a,
              x='rare choose', y=test_value)
    # sn.regplot(data=a, ci=None,
    #           x='uncertanty', y=test_value)
    plt.show()
    print(i)
    display(pearsonr(a[test_value], a['rare choose']))

"""# anova part 2.1 & 2.2"""

# dfs2_1_accurcy_melt_3_way['exp'] = [2.1]*len(dfs2_1_accurcy_melt_3_way)
# dfs2_2_accurcy_melt_3_way['exp'] = [2.2]*len(dfs2_2_accurcy_melt_3_way)
x_2_1['exp'] = [2.1]*len(x_2_1)
x_2_2['exp'] = [2.2]*len(x_2_2)
connect_data = pd.concat([x_2_1, x_2_2])
aov = AnovaRM(
    connect_data,
    depvar='value',
    subject="participant",
    within=[congruent, infrequnce_error, 'exp']
    ).fit()
print(aov)

sn.catplot(kind='box',
           data=connect_data,
           x='exp', col=infrequnce_error, hue='congruent', y='value')
hue_order = ['true_answer', 'infreq_answer', 'wides_answer']
sn.catplot(kind='box',
           data=dfs2_1_accurcy_melt_3_way,
           x='common', col=infrequnce_error, hue='congruent', col_order=hue_order, y='value')
sn.catplot(kind='box',
           data=dfs2_2_accurcy_melt_3_way,
          x='common', col=infrequnce_error,hue='congruent',col_order=hue_order, y='value')

display((dfs2_1_accurcy_melt_3_way[dfs2_1_accurcy_melt_3_way[infrequnce_error] == 'true_answer']).mean())
display((dfs2_2_accurcy_melt_3_way[dfs2_2_accurcy_melt_3_way[infrequnce_error] == 'true_answer']).mean())

"""# just a test"""

#describe of the data
#display(dfs1_time_melt.groupby(["participant" ,'common', "congruent"])["value"].describe())
# graph for each praticipant
#sn.catplot(kind='box',data=dfs1_time_melt, x='participant',aspect=1, hue="congruent", col='common', y='value')

#kind='violin'
#kind='strip'
# sn.catplot(kind='point', join=False,
#             data=dfs1_time_melt, x="congruent", hue='common', y='value',units='participant')
# sn.catplot(kind='violin',data=dfs1_time_melt, x="congruent", col='common', y='value')

# sns.distplot( mydata[mydata.gender ==  '0' ].weight6weeks, ax = ax, label =  'Female' ) 

plt.show()

"""# Create pivot table

## Create pivot table part 1
"""

# def pivot_table_for_part_1(dfs):
#     pivot_cerainty = dfs[(dfs[stroop_corr] == 1) & (dfs['test_certainty_anser_currect'] == 1) &
#                                   (dfs["test_certainty_mouse.time"] != 0)].pivot_table(
#             index=["participant"],
#             columns=[congruent, common], values=["test_certainty_mouse.time"],
#             aggfunc=[np.mean])
#     file_to_write = {part_1:[pivot_cerainty, "part_1"]}
#     return file_to_write
# file_to_write_part_1 = pivot_table_for_part_1(dfs1)
# file_to_write_part_1[part_1][0].head(5)

"""## add error of infrequnce"""

# def add_error_of_infrequnce(file):
#     file[infrequnce_error] = np.select([(file[experiment] == "certainty") &
#                                         ((file[click_name] == "two_up")
#                                          | (file[click_name] == "one_up") | (file[click_name] == "three_up") |
#                                          (file[click_name] == "four_up") | (file[click_name] == "one_down")
#                                          | (file[click_name] == "two_down") | (file[click_name] == "three_down"))],
#                                        [True], default=False)
#     file = file[(file[stroop_corr] == 1) & (file[infrequnce_error] == True)] \
#         .pivot_table(index=["participant"], columns=[congruent], values=[], aggfunc=[len])
#     file_new = pd.DataFrame([])
#     file_new[0] = file.xs((0), level='congruent', axis=1).sum(axis=1) if ('len', 0) in file.columns else 0
#     file_new[1] = file.xs((1), level='congruent', axis=1).sum(axis=1) if ('len', 1) in file.columns else 0
#     return file_new

"""## create pivot table - part 2"""

# def pivot_table_for_answer_currect_congruent_common(file):
#     return file.pivot_table(
#                 index=["participant"],
#                 columns=[answer_test_currect,congruent, common], values=[],
#         aggfunc=[len])
    
# def pivot_table_for_part_2(dfs):
#     pivot_cerainty = pivot_table_for_answer_currect_congruent_common(
#             dfs[(dfs[experiment] == "certainty") & (dfs[stroop_corr] == 1)])
#     pivot_error = add_error_of_infrequnce(dfs)
#     pivot_uncerainty = pivot_table_for_answer_currect_congruent_common(
#             dfs[(dfs[experiment] == "uncertainty") & (dfs[stroop_corr] == 1)])
#     pivot_click_on_not_image = pivot_table_for_answer_currect_congruent_common(
#             dfs[(dfs[experiment] == "uncertainty") & (dfs[stroop_corr] == 1) &
#                  (dfs["test_mouse_anser_rual.clicked_name"] == "three_right")])
#     file_to_write = {not_image_file_part_2: [pivot_click_on_not_image, "click_on_not_was_image"],
#                      cerainty_file_part_2: [pivot_cerainty, "certainty"],
#                      uncertainty_file_part_2: [pivot_uncerainty, "uncertainty"]}
#     return file_to_write, pivot_error
# file_to_write_part_2, pivot_error = pivot_table_for_part_2(dfs2)
# file_to_write_part_2[not_image_file_part_2][0].head(5)

#file_to_write_part_2[cerainty_file_part_2][0].head(5)

# file_to_write_part_2[uncertainty_file_part_2][0].head(5)

# pivot_error.head(5)

# from scipy import stats 
# import statsmodels.formula.api as smf
# from statsmodels.stats.anova import AnovaRM
# my_model_fit = smf.mixedlm("value ~ common * congruent", dfs1_time_melt, groups=dfs1_time_melt["participant"]).fit()
# my_model_fit.random_effects
# display(my_model_fit.summary())
# AnovaRM(dfs1_time_melt, 'value', 'participant', within=['common', 'congruent']).fit().anova_table

# # res = stat()
# # res.anova_stat(df=dfs1_accurcy_melt, res_var='value', anova_model='value ~ C(common) + C(congruent) + \
# # C(common):C(congruent)')
# # res.anova_summary

# def errre(result_1, result_2):
#   mean1, mean2 = np.mean(result_1), np.mean(result_2)
#   n = len(result_1)
#   # sum squared difference between observations
#   d1 = sum((result_1-result_2)**2)
#   # sum difference between observations
#   d2 = sum(result_1-result_2)
#   std_dev = np.sqrt((d1 - (d2**2 / n)) / (n - 1))
#   # standard error of the difference between a mean
#   se = std_dev / np.sqrt(n)
#   print(std_dev)
#   t_stat = (mean1 - mean2) / se
#   df = n - 1
#   # calculate the critical value
#   critical =stats.t.ppf(0.95, df)
#   p = (1.0 - stats.t.cdf(abs(t_stat), df)) * 2.0
#   print(t_stat,critical,p)
# errre(a, b)